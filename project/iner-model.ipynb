{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended a10_s1_t1!\n",
      "appended a10_s1_t2!\n",
      "appended a10_s1_t3!\n",
      "appended a10_s1_t4!\n",
      "appended a10_s2_t1!\n",
      "appended a10_s2_t2!\n",
      "appended a10_s2_t3!\n",
      "appended a10_s2_t4!\n",
      "appended a10_s3_t1!\n",
      "appended a10_s3_t2!\n",
      "appended a10_s3_t3!\n",
      "appended a10_s3_t4!\n",
      "appended a10_s4_t1!\n",
      "appended a10_s4_t2!\n",
      "appended a10_s4_t3!\n",
      "appended a10_s4_t4!\n",
      "appended a10_s5_t1!\n",
      "appended a10_s5_t2!\n",
      "appended a10_s5_t3!\n",
      "appended a10_s5_t4!\n",
      "appended a10_s6_t1!\n",
      "appended a10_s6_t2!\n",
      "appended a10_s6_t3!\n",
      "appended a10_s6_t4!\n",
      "appended a10_s7_t1!\n",
      "appended a10_s7_t2!\n",
      "appended a10_s7_t3!\n",
      "appended a10_s7_t4!\n",
      "appended a10_s8_t1!\n",
      "appended a10_s8_t2!\n",
      "appended a10_s8_t3!\n",
      "appended a10_s8_t4!\n",
      "appended a11_s1_t1!\n",
      "appended a11_s1_t2!\n",
      "appended a11_s1_t3!\n",
      "appended a11_s1_t4!\n",
      "appended a11_s2_t1!\n",
      "appended a11_s2_t2!\n",
      "appended a11_s2_t3!\n",
      "appended a11_s2_t4!\n",
      "appended a11_s3_t1!\n",
      "appended a11_s3_t2!\n",
      "appended a11_s3_t3!\n",
      "appended a11_s3_t4!\n",
      "appended a11_s4_t1!\n",
      "appended a11_s4_t2!\n",
      "appended a11_s4_t3!\n",
      "appended a11_s4_t4!\n",
      "appended a11_s5_t1!\n",
      "appended a11_s5_t2!\n",
      "appended a11_s5_t3!\n",
      "appended a11_s5_t4!\n",
      "appended a11_s6_t1!\n",
      "appended a11_s6_t2!\n",
      "appended a11_s6_t3!\n",
      "appended a11_s6_t4!\n",
      "appended a11_s7_t1!\n",
      "appended a11_s7_t2!\n",
      "appended a11_s7_t3!\n",
      "appended a11_s7_t4!\n",
      "appended a11_s8_t1!\n",
      "appended a11_s8_t2!\n",
      "appended a11_s8_t3!\n",
      "appended a11_s8_t4!\n",
      "appended a12_s1_t1!\n",
      "appended a12_s1_t2!\n",
      "appended a12_s1_t3!\n",
      "appended a12_s1_t4!\n",
      "appended a12_s2_t1!\n",
      "appended a12_s2_t2!\n",
      "appended a12_s2_t3!\n",
      "appended a12_s2_t4!\n",
      "appended a12_s3_t1!\n",
      "appended a12_s3_t2!\n",
      "appended a12_s3_t3!\n",
      "appended a12_s3_t4!\n",
      "appended a12_s4_t1!\n",
      "appended a12_s4_t2!\n",
      "appended a12_s4_t3!\n",
      "appended a12_s4_t4!\n",
      "appended a12_s5_t1!\n",
      "appended a12_s5_t2!\n",
      "appended a12_s5_t3!\n",
      "appended a12_s5_t4!\n",
      "appended a12_s6_t1!\n",
      "appended a12_s6_t2!\n",
      "appended a12_s6_t3!\n",
      "appended a12_s6_t4!\n",
      "appended a12_s7_t1!\n",
      "appended a12_s7_t2!\n",
      "appended a12_s7_t3!\n",
      "appended a12_s7_t4!\n",
      "appended a12_s8_t1!\n",
      "appended a12_s8_t2!\n",
      "appended a12_s8_t3!\n",
      "appended a12_s8_t4!\n",
      "appended a13_s1_t1!\n",
      "appended a13_s1_t2!\n",
      "appended a13_s1_t3!\n",
      "appended a13_s1_t4!\n",
      "appended a13_s2_t1!\n",
      "appended a13_s2_t2!\n",
      "appended a13_s2_t3!\n",
      "appended a13_s2_t4!\n",
      "appended a13_s3_t1!\n",
      "appended a13_s3_t2!\n",
      "appended a13_s3_t3!\n",
      "appended a13_s3_t4!\n",
      "appended a13_s4_t1!\n",
      "appended a13_s4_t2!\n",
      "appended a13_s4_t3!\n",
      "appended a13_s4_t4!\n",
      "appended a13_s5_t1!\n",
      "appended a13_s5_t2!\n",
      "appended a13_s5_t3!\n",
      "appended a13_s5_t4!\n",
      "appended a13_s6_t1!\n",
      "appended a13_s6_t2!\n",
      "appended a13_s6_t3!\n",
      "appended a13_s6_t4!\n",
      "appended a13_s7_t1!\n",
      "appended a13_s7_t2!\n",
      "appended a13_s7_t3!\n",
      "appended a13_s7_t4!\n",
      "appended a13_s8_t1!\n",
      "appended a13_s8_t2!\n",
      "appended a13_s8_t3!\n",
      "appended a13_s8_t4!\n",
      "appended a14_s1_t1!\n",
      "appended a14_s1_t2!\n",
      "appended a14_s1_t3!\n",
      "appended a14_s1_t4!\n",
      "appended a14_s2_t1!\n",
      "appended a14_s2_t2!\n",
      "appended a14_s2_t3!\n",
      "appended a14_s2_t4!\n",
      "appended a14_s3_t1!\n",
      "appended a14_s3_t2!\n",
      "appended a14_s3_t3!\n",
      "appended a14_s3_t4!\n",
      "appended a14_s4_t1!\n",
      "appended a14_s4_t2!\n",
      "appended a14_s4_t3!\n",
      "appended a14_s4_t4!\n",
      "appended a14_s5_t1!\n",
      "appended a14_s5_t2!\n",
      "appended a14_s5_t3!\n",
      "appended a14_s5_t4!\n",
      "appended a14_s6_t1!\n",
      "appended a14_s6_t2!\n",
      "appended a14_s6_t3!\n",
      "appended a14_s6_t4!\n",
      "appended a14_s7_t1!\n",
      "appended a14_s7_t2!\n",
      "appended a14_s7_t3!\n",
      "appended a14_s7_t4!\n",
      "appended a14_s8_t1!\n",
      "appended a14_s8_t2!\n",
      "appended a14_s8_t3!\n",
      "appended a14_s8_t4!\n",
      "appended a15_s1_t1!\n",
      "appended a15_s1_t2!\n",
      "appended a15_s1_t3!\n",
      "appended a15_s1_t4!\n",
      "appended a15_s2_t1!\n",
      "appended a15_s2_t2!\n",
      "appended a15_s2_t3!\n",
      "appended a15_s2_t4!\n",
      "appended a15_s3_t1!\n",
      "appended a15_s3_t2!\n",
      "appended a15_s3_t3!\n",
      "appended a15_s3_t4!\n",
      "appended a15_s4_t1!\n",
      "appended a15_s4_t2!\n",
      "appended a15_s4_t3!\n",
      "appended a15_s4_t4!\n",
      "appended a15_s5_t1!\n",
      "appended a15_s5_t2!\n",
      "appended a15_s5_t3!\n",
      "appended a15_s5_t4!\n",
      "appended a15_s6_t1!\n",
      "appended a15_s6_t2!\n",
      "appended a15_s6_t3!\n",
      "appended a15_s6_t4!\n",
      "appended a15_s7_t1!\n",
      "appended a15_s7_t2!\n",
      "appended a15_s7_t3!\n",
      "appended a15_s7_t4!\n",
      "appended a15_s8_t1!\n",
      "appended a15_s8_t2!\n",
      "appended a15_s8_t3!\n",
      "appended a15_s8_t4!\n",
      "appended a16_s1_t1!\n",
      "appended a16_s1_t2!\n",
      "appended a16_s1_t3!\n",
      "appended a16_s1_t4!\n",
      "appended a16_s2_t1!\n",
      "appended a16_s2_t2!\n",
      "appended a16_s2_t3!\n",
      "appended a16_s2_t4!\n",
      "appended a16_s3_t1!\n",
      "appended a16_s3_t2!\n",
      "appended a16_s3_t3!\n",
      "appended a16_s3_t4!\n",
      "appended a16_s4_t1!\n",
      "appended a16_s4_t2!\n",
      "appended a16_s4_t3!\n",
      "appended a16_s4_t4!\n",
      "appended a16_s5_t1!\n",
      "appended a16_s5_t2!\n",
      "appended a16_s5_t3!\n",
      "appended a16_s5_t4!\n",
      "appended a16_s6_t1!\n",
      "appended a16_s6_t2!\n",
      "appended a16_s6_t3!\n",
      "appended a16_s6_t4!\n",
      "appended a16_s7_t1!\n",
      "appended a16_s7_t2!\n",
      "appended a16_s7_t3!\n",
      "appended a16_s7_t4!\n",
      "appended a16_s8_t1!\n",
      "appended a16_s8_t2!\n",
      "appended a16_s8_t3!\n",
      "appended a16_s8_t4!\n",
      "appended a17_s1_t1!\n",
      "appended a17_s1_t2!\n",
      "appended a17_s1_t3!\n",
      "appended a17_s1_t4!\n",
      "appended a17_s2_t1!\n",
      "appended a17_s2_t2!\n",
      "appended a17_s2_t3!\n",
      "appended a17_s2_t4!\n",
      "appended a17_s3_t1!\n",
      "appended a17_s3_t2!\n",
      "appended a17_s3_t3!\n",
      "appended a17_s3_t4!\n",
      "appended a17_s4_t1!\n",
      "appended a17_s4_t2!\n",
      "appended a17_s4_t3!\n",
      "appended a17_s4_t4!\n",
      "appended a17_s5_t1!\n",
      "appended a17_s5_t2!\n",
      "appended a17_s5_t3!\n",
      "appended a17_s5_t4!\n",
      "appended a17_s6_t1!\n",
      "appended a17_s6_t2!\n",
      "appended a17_s6_t3!\n",
      "appended a17_s6_t4!\n",
      "appended a17_s7_t1!\n",
      "appended a17_s7_t2!\n",
      "appended a17_s7_t3!\n",
      "appended a17_s7_t4!\n",
      "appended a17_s8_t1!\n",
      "appended a17_s8_t2!\n",
      "appended a17_s8_t3!\n",
      "appended a17_s8_t4!\n",
      "appended a18_s1_t1!\n",
      "appended a18_s1_t2!\n",
      "appended a18_s1_t3!\n",
      "appended a18_s1_t4!\n",
      "appended a18_s2_t1!\n",
      "appended a18_s2_t2!\n",
      "appended a18_s2_t3!\n",
      "appended a18_s2_t4!\n",
      "appended a18_s3_t1!\n",
      "appended a18_s3_t2!\n",
      "appended a18_s3_t3!\n",
      "appended a18_s3_t4!\n",
      "appended a18_s4_t1!\n",
      "appended a18_s4_t2!\n",
      "appended a18_s4_t3!\n",
      "appended a18_s4_t4!\n",
      "appended a18_s5_t1!\n",
      "appended a18_s5_t2!\n",
      "appended a18_s5_t3!\n",
      "appended a18_s5_t4!\n",
      "appended a18_s6_t1!\n",
      "appended a18_s6_t2!\n",
      "appended a18_s6_t3!\n",
      "appended a18_s6_t4!\n",
      "appended a18_s7_t1!\n",
      "appended a18_s7_t2!\n",
      "appended a18_s7_t3!\n",
      "appended a18_s7_t4!\n",
      "appended a18_s8_t1!\n",
      "appended a18_s8_t2!\n",
      "appended a18_s8_t3!\n",
      "appended a18_s8_t4!\n",
      "appended a19_s1_t1!\n",
      "appended a19_s1_t2!\n",
      "appended a19_s1_t3!\n",
      "appended a19_s1_t4!\n",
      "appended a19_s2_t1!\n",
      "appended a19_s2_t2!\n",
      "appended a19_s2_t3!\n",
      "appended a19_s2_t4!\n",
      "appended a19_s3_t1!\n",
      "appended a19_s3_t2!\n",
      "appended a19_s3_t3!\n",
      "appended a19_s3_t4!\n",
      "appended a19_s4_t1!\n",
      "appended a19_s4_t2!\n",
      "appended a19_s4_t3!\n",
      "appended a19_s4_t4!\n",
      "appended a19_s5_t1!\n",
      "appended a19_s5_t2!\n",
      "appended a19_s5_t3!\n",
      "appended a19_s5_t4!\n",
      "appended a19_s6_t1!\n",
      "appended a19_s6_t2!\n",
      "appended a19_s6_t3!\n",
      "appended a19_s6_t4!\n",
      "appended a19_s7_t1!\n",
      "appended a19_s7_t2!\n",
      "appended a19_s7_t3!\n",
      "appended a19_s7_t4!\n",
      "appended a19_s8_t1!\n",
      "appended a19_s8_t2!\n",
      "appended a19_s8_t3!\n",
      "appended a19_s8_t4!\n",
      "appended a1_s1_t1!\n",
      "appended a1_s1_t2!\n",
      "appended a1_s1_t3!\n",
      "appended a1_s1_t4!\n",
      "appended a1_s2_t1!\n",
      "appended a1_s2_t2!\n",
      "appended a1_s2_t3!\n",
      "appended a1_s2_t4!\n",
      "appended a1_s3_t1!\n",
      "appended a1_s3_t2!\n",
      "appended a1_s3_t3!\n",
      "appended a1_s3_t4!\n",
      "appended a1_s4_t1!\n",
      "appended a1_s4_t2!\n",
      "appended a1_s4_t3!\n",
      "appended a1_s4_t4!\n",
      "appended a1_s5_t1!\n",
      "appended a1_s5_t2!\n",
      "appended a1_s5_t3!\n",
      "appended a1_s5_t4!\n",
      "appended a1_s6_t1!\n",
      "appended a1_s6_t2!\n",
      "appended a1_s6_t3!\n",
      "appended a1_s6_t4!\n",
      "appended a1_s7_t1!\n",
      "appended a1_s7_t2!\n",
      "appended a1_s7_t3!\n",
      "appended a1_s7_t4!\n",
      "appended a1_s8_t1!\n",
      "appended a1_s8_t2!\n",
      "appended a1_s8_t3!\n",
      "appended a1_s8_t4!\n",
      "appended a20_s1_t1!\n",
      "appended a20_s1_t2!\n",
      "appended a20_s1_t3!\n",
      "appended a20_s1_t4!\n",
      "appended a20_s2_t1!\n",
      "appended a20_s2_t2!\n",
      "appended a20_s2_t3!\n",
      "appended a20_s2_t4!\n",
      "appended a20_s3_t1!\n",
      "appended a20_s3_t2!\n",
      "appended a20_s3_t3!\n",
      "appended a20_s3_t4!\n",
      "appended a20_s4_t1!\n",
      "appended a20_s4_t2!\n",
      "appended a20_s4_t3!\n",
      "appended a20_s4_t4!\n",
      "appended a20_s5_t1!\n",
      "appended a20_s5_t2!\n",
      "appended a20_s5_t3!\n",
      "appended a20_s5_t4!\n",
      "appended a20_s6_t1!\n",
      "appended a20_s6_t2!\n",
      "appended a20_s6_t3!\n",
      "appended a20_s6_t4!\n",
      "appended a20_s7_t1!\n",
      "appended a20_s7_t2!\n",
      "appended a20_s7_t3!\n",
      "appended a20_s7_t4!\n",
      "appended a20_s8_t1!\n",
      "appended a20_s8_t2!\n",
      "appended a20_s8_t3!\n",
      "appended a20_s8_t4!\n",
      "appended a21_s1_t1!\n",
      "appended a21_s1_t2!\n",
      "appended a21_s1_t3!\n",
      "appended a21_s1_t4!\n",
      "appended a21_s2_t1!\n",
      "appended a21_s2_t2!\n",
      "appended a21_s2_t3!\n",
      "appended a21_s2_t4!\n",
      "appended a21_s3_t1!\n",
      "appended a21_s3_t2!\n",
      "appended a21_s3_t3!\n",
      "appended a21_s3_t4!\n",
      "appended a21_s4_t1!\n",
      "appended a21_s4_t2!\n",
      "appended a21_s4_t3!\n",
      "appended a21_s4_t4!\n",
      "appended a21_s5_t1!\n",
      "appended a21_s5_t2!\n",
      "appended a21_s5_t3!\n",
      "appended a21_s5_t4!\n",
      "appended a21_s6_t1!\n",
      "appended a21_s6_t2!\n",
      "appended a21_s6_t3!\n",
      "appended a21_s6_t4!\n",
      "appended a21_s7_t1!\n",
      "appended a21_s7_t2!\n",
      "appended a21_s7_t3!\n",
      "appended a21_s7_t4!\n",
      "appended a21_s8_t1!\n",
      "appended a21_s8_t2!\n",
      "appended a21_s8_t3!\n",
      "appended a21_s8_t4!\n",
      "appended a22_s1_t1!\n",
      "appended a22_s1_t2!\n",
      "appended a22_s1_t3!\n",
      "appended a22_s1_t4!\n",
      "appended a22_s2_t1!\n",
      "appended a22_s2_t2!\n",
      "appended a22_s2_t3!\n",
      "appended a22_s2_t4!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended a22_s3_t1!\n",
      "appended a22_s3_t2!\n",
      "appended a22_s3_t3!\n",
      "appended a22_s3_t4!\n",
      "appended a22_s4_t1!\n",
      "appended a22_s4_t2!\n",
      "appended a22_s4_t3!\n",
      "appended a22_s4_t4!\n",
      "appended a22_s5_t1!\n",
      "appended a22_s5_t2!\n",
      "appended a22_s5_t3!\n",
      "appended a22_s5_t4!\n",
      "appended a22_s6_t1!\n",
      "appended a22_s6_t2!\n",
      "appended a22_s6_t3!\n",
      "appended a22_s6_t4!\n",
      "appended a22_s7_t1!\n",
      "appended a22_s7_t2!\n",
      "appended a22_s7_t3!\n",
      "appended a22_s7_t4!\n",
      "appended a22_s8_t1!\n",
      "appended a22_s8_t2!\n",
      "appended a22_s8_t3!\n",
      "appended a22_s8_t4!\n",
      "appended a23_s1_t1!\n",
      "appended a23_s1_t2!\n",
      "appended a23_s1_t3!\n",
      "appended a23_s1_t4!\n",
      "appended a23_s2_t1!\n",
      "appended a23_s2_t2!\n",
      "appended a23_s2_t3!\n",
      "appended a23_s2_t4!\n",
      "appended a23_s3_t1!\n",
      "appended a23_s3_t2!\n",
      "appended a23_s3_t3!\n",
      "appended a23_s3_t4!\n",
      "appended a23_s4_t1!\n",
      "appended a23_s4_t2!\n",
      "appended a23_s4_t3!\n",
      "appended a23_s4_t4!\n",
      "appended a23_s5_t1!\n",
      "appended a23_s5_t2!\n",
      "appended a23_s5_t3!\n",
      "appended a23_s5_t4!\n",
      "appended a23_s6_t1!\n",
      "appended a23_s6_t2!\n",
      "appended a23_s6_t3!\n",
      "appended a23_s7_t1!\n",
      "appended a23_s7_t2!\n",
      "appended a23_s7_t3!\n",
      "appended a23_s7_t4!\n",
      "appended a23_s8_t1!\n",
      "appended a23_s8_t2!\n",
      "appended a23_s8_t3!\n",
      "appended a23_s8_t4!\n",
      "appended a24_s1_t1!\n",
      "appended a24_s1_t2!\n",
      "appended a24_s1_t3!\n",
      "appended a24_s1_t4!\n",
      "appended a24_s2_t1!\n",
      "appended a24_s2_t2!\n",
      "appended a24_s2_t3!\n",
      "appended a24_s2_t4!\n",
      "appended a24_s3_t1!\n",
      "appended a24_s3_t2!\n",
      "appended a24_s3_t3!\n",
      "appended a24_s3_t4!\n",
      "appended a24_s4_t1!\n",
      "appended a24_s4_t2!\n",
      "appended a24_s4_t3!\n",
      "appended a24_s4_t4!\n",
      "appended a24_s5_t1!\n",
      "appended a24_s5_t2!\n",
      "appended a24_s5_t3!\n",
      "appended a24_s5_t4!\n",
      "appended a24_s6_t1!\n",
      "appended a24_s6_t2!\n",
      "appended a24_s6_t3!\n",
      "appended a24_s6_t4!\n",
      "appended a24_s7_t1!\n",
      "appended a24_s7_t2!\n",
      "appended a24_s7_t3!\n",
      "appended a24_s7_t4!\n",
      "appended a24_s8_t1!\n",
      "appended a24_s8_t2!\n",
      "appended a24_s8_t3!\n",
      "appended a24_s8_t4!\n",
      "appended a25_s1_t1!\n",
      "appended a25_s1_t2!\n",
      "appended a25_s1_t3!\n",
      "appended a25_s1_t4!\n",
      "appended a25_s2_t1!\n",
      "appended a25_s2_t2!\n",
      "appended a25_s2_t3!\n",
      "appended a25_s2_t4!\n",
      "appended a25_s3_t1!\n",
      "appended a25_s3_t2!\n",
      "appended a25_s3_t3!\n",
      "appended a25_s3_t4!\n",
      "appended a25_s4_t1!\n",
      "appended a25_s4_t2!\n",
      "appended a25_s4_t3!\n",
      "appended a25_s4_t4!\n",
      "appended a25_s5_t1!\n",
      "appended a25_s5_t2!\n",
      "appended a25_s5_t3!\n",
      "appended a25_s5_t4!\n",
      "appended a25_s6_t1!\n",
      "appended a25_s6_t2!\n",
      "appended a25_s6_t3!\n",
      "appended a25_s6_t4!\n",
      "appended a25_s7_t1!\n",
      "appended a25_s7_t2!\n",
      "appended a25_s7_t3!\n",
      "appended a25_s7_t4!\n",
      "appended a25_s8_t1!\n",
      "appended a25_s8_t2!\n",
      "appended a25_s8_t3!\n",
      "appended a25_s8_t4!\n",
      "appended a26_s1_t1!\n",
      "appended a26_s1_t2!\n",
      "appended a26_s1_t3!\n",
      "appended a26_s1_t4!\n",
      "appended a26_s2_t1!\n",
      "appended a26_s2_t2!\n",
      "appended a26_s2_t3!\n",
      "appended a26_s2_t4!\n",
      "appended a26_s3_t1!\n",
      "appended a26_s3_t2!\n",
      "appended a26_s3_t3!\n",
      "appended a26_s3_t4!\n",
      "appended a26_s4_t1!\n",
      "appended a26_s4_t2!\n",
      "appended a26_s4_t3!\n",
      "appended a26_s4_t4!\n",
      "appended a26_s5_t1!\n",
      "appended a26_s5_t2!\n",
      "appended a26_s5_t3!\n",
      "appended a26_s5_t4!\n",
      "appended a26_s6_t1!\n",
      "appended a26_s6_t2!\n",
      "appended a26_s6_t3!\n",
      "appended a26_s6_t4!\n",
      "appended a26_s7_t1!\n",
      "appended a26_s7_t2!\n",
      "appended a26_s7_t3!\n",
      "appended a26_s7_t4!\n",
      "appended a26_s8_t1!\n",
      "appended a26_s8_t2!\n",
      "appended a26_s8_t3!\n",
      "appended a26_s8_t4!\n",
      "appended a27_s1_t1!\n",
      "appended a27_s1_t2!\n",
      "appended a27_s1_t3!\n",
      "appended a27_s1_t4!\n",
      "appended a27_s2_t1!\n",
      "appended a27_s2_t2!\n",
      "appended a27_s2_t3!\n",
      "appended a27_s2_t4!\n",
      "appended a27_s3_t1!\n",
      "appended a27_s3_t2!\n",
      "appended a27_s3_t3!\n",
      "appended a27_s3_t4!\n",
      "appended a27_s4_t1!\n",
      "appended a27_s4_t2!\n",
      "appended a27_s4_t3!\n",
      "appended a27_s4_t4!\n",
      "appended a27_s5_t1!\n",
      "appended a27_s5_t2!\n",
      "appended a27_s5_t3!\n",
      "appended a27_s5_t4!\n",
      "appended a27_s6_t1!\n",
      "appended a27_s6_t2!\n",
      "appended a27_s6_t3!\n",
      "appended a27_s6_t4!\n",
      "appended a27_s7_t1!\n",
      "appended a27_s7_t2!\n",
      "appended a27_s7_t3!\n",
      "appended a27_s7_t4!\n",
      "appended a27_s8_t1!\n",
      "appended a27_s8_t2!\n",
      "appended a27_s8_t3!\n",
      "appended a2_s1_t1!\n",
      "appended a2_s1_t2!\n",
      "appended a2_s1_t3!\n",
      "appended a2_s1_t4!\n",
      "appended a2_s2_t1!\n",
      "appended a2_s2_t2!\n",
      "appended a2_s2_t3!\n",
      "appended a2_s2_t4!\n",
      "appended a2_s3_t1!\n",
      "appended a2_s3_t2!\n",
      "appended a2_s3_t3!\n",
      "appended a2_s3_t4!\n",
      "appended a2_s4_t1!\n",
      "appended a2_s4_t2!\n",
      "appended a2_s4_t3!\n",
      "appended a2_s4_t4!\n",
      "appended a2_s5_t1!\n",
      "appended a2_s5_t2!\n",
      "appended a2_s5_t3!\n",
      "appended a2_s5_t4!\n",
      "appended a2_s6_t1!\n",
      "appended a2_s6_t2!\n",
      "appended a2_s6_t3!\n",
      "appended a2_s6_t4!\n",
      "appended a2_s7_t1!\n",
      "appended a2_s7_t2!\n",
      "appended a2_s7_t3!\n",
      "appended a2_s7_t4!\n",
      "appended a2_s8_t1!\n",
      "appended a2_s8_t2!\n",
      "appended a2_s8_t3!\n",
      "appended a2_s8_t4!\n",
      "appended a3_s1_t1!\n",
      "appended a3_s1_t2!\n",
      "appended a3_s1_t3!\n",
      "appended a3_s1_t4!\n",
      "appended a3_s2_t1!\n",
      "appended a3_s2_t2!\n",
      "appended a3_s2_t3!\n",
      "appended a3_s2_t4!\n",
      "appended a3_s3_t1!\n",
      "appended a3_s3_t2!\n",
      "appended a3_s3_t3!\n",
      "appended a3_s3_t4!\n",
      "appended a3_s4_t1!\n",
      "appended a3_s4_t2!\n",
      "appended a3_s4_t3!\n",
      "appended a3_s4_t4!\n",
      "appended a3_s5_t1!\n",
      "appended a3_s5_t2!\n",
      "appended a3_s5_t3!\n",
      "appended a3_s5_t4!\n",
      "appended a3_s6_t1!\n",
      "appended a3_s6_t2!\n",
      "appended a3_s6_t3!\n",
      "appended a3_s6_t4!\n",
      "appended a3_s7_t1!\n",
      "appended a3_s7_t2!\n",
      "appended a3_s7_t3!\n",
      "appended a3_s7_t4!\n",
      "appended a3_s8_t1!\n",
      "appended a3_s8_t2!\n",
      "appended a3_s8_t3!\n",
      "appended a3_s8_t4!\n",
      "appended a4_s1_t1!\n",
      "appended a4_s1_t2!\n",
      "appended a4_s1_t3!\n",
      "appended a4_s1_t4!\n",
      "appended a4_s2_t1!\n",
      "appended a4_s2_t2!\n",
      "appended a4_s2_t3!\n",
      "appended a4_s2_t4!\n",
      "appended a4_s3_t1!\n",
      "appended a4_s3_t2!\n",
      "appended a4_s3_t3!\n",
      "appended a4_s3_t4!\n",
      "appended a4_s4_t1!\n",
      "appended a4_s4_t2!\n",
      "appended a4_s4_t3!\n",
      "appended a4_s4_t4!\n",
      "appended a4_s5_t1!\n",
      "appended a4_s5_t2!\n",
      "appended a4_s5_t3!\n",
      "appended a4_s5_t4!\n",
      "appended a4_s6_t1!\n",
      "appended a4_s6_t2!\n",
      "appended a4_s6_t3!\n",
      "appended a4_s6_t4!\n",
      "appended a4_s7_t1!\n",
      "appended a4_s7_t2!\n",
      "appended a4_s7_t3!\n",
      "appended a4_s7_t4!\n",
      "appended a4_s8_t1!\n",
      "appended a4_s8_t2!\n",
      "appended a4_s8_t3!\n",
      "appended a4_s8_t4!\n",
      "appended a5_s1_t1!\n",
      "appended a5_s1_t2!\n",
      "appended a5_s1_t3!\n",
      "appended a5_s1_t4!\n",
      "appended a5_s2_t1!\n",
      "appended a5_s2_t2!\n",
      "appended a5_s2_t3!\n",
      "appended a5_s2_t4!\n",
      "appended a5_s3_t1!\n",
      "appended a5_s3_t2!\n",
      "appended a5_s3_t3!\n",
      "appended a5_s3_t4!\n",
      "appended a5_s4_t1!\n",
      "appended a5_s4_t2!\n",
      "appended a5_s4_t3!\n",
      "appended a5_s4_t4!\n",
      "appended a5_s5_t1!\n",
      "appended a5_s5_t2!\n",
      "appended a5_s5_t3!\n",
      "appended a5_s5_t4!\n",
      "appended a5_s6_t1!\n",
      "appended a5_s6_t2!\n",
      "appended a5_s6_t3!\n",
      "appended a5_s6_t4!\n",
      "appended a5_s7_t1!\n",
      "appended a5_s7_t2!\n",
      "appended a5_s7_t3!\n",
      "appended a5_s7_t4!\n",
      "appended a5_s8_t1!\n",
      "appended a5_s8_t2!\n",
      "appended a5_s8_t3!\n",
      "appended a5_s8_t4!\n",
      "appended a6_s1_t1!\n",
      "appended a6_s1_t2!\n",
      "appended a6_s1_t3!\n",
      "appended a6_s1_t4!\n",
      "appended a6_s2_t1!\n",
      "appended a6_s2_t2!\n",
      "appended a6_s2_t3!\n",
      "appended a6_s2_t4!\n",
      "appended a6_s3_t1!\n",
      "appended a6_s3_t2!\n",
      "appended a6_s3_t3!\n",
      "appended a6_s3_t4!\n",
      "appended a6_s4_t1!\n",
      "appended a6_s4_t2!\n",
      "appended a6_s4_t3!\n",
      "appended a6_s4_t4!\n",
      "appended a6_s5_t1!\n",
      "appended a6_s5_t2!\n",
      "appended a6_s5_t3!\n",
      "appended a6_s5_t4!\n",
      "appended a6_s6_t1!\n",
      "appended a6_s6_t2!\n",
      "appended a6_s6_t3!\n",
      "appended a6_s6_t4!\n",
      "appended a6_s7_t1!\n",
      "appended a6_s7_t2!\n",
      "appended a6_s7_t3!\n",
      "appended a6_s7_t4!\n",
      "appended a6_s8_t1!\n",
      "appended a6_s8_t2!\n",
      "appended a6_s8_t3!\n",
      "appended a6_s8_t4!\n",
      "appended a7_s1_t1!\n",
      "appended a7_s1_t2!\n",
      "appended a7_s1_t3!\n",
      "appended a7_s1_t4!\n",
      "appended a7_s2_t1!\n",
      "appended a7_s2_t2!\n",
      "appended a7_s2_t3!\n",
      "appended a7_s2_t4!\n",
      "appended a7_s3_t1!\n",
      "appended a7_s3_t2!\n",
      "appended a7_s3_t3!\n",
      "appended a7_s3_t4!\n",
      "appended a7_s4_t1!\n",
      "appended a7_s4_t2!\n",
      "appended a7_s4_t3!\n",
      "appended a7_s4_t4!\n",
      "appended a7_s5_t1!\n",
      "appended a7_s5_t2!\n",
      "appended a7_s5_t3!\n",
      "appended a7_s5_t4!\n",
      "appended a7_s6_t1!\n",
      "appended a7_s6_t2!\n",
      "appended a7_s6_t3!\n",
      "appended a7_s6_t4!\n",
      "appended a7_s7_t1!\n",
      "appended a7_s7_t2!\n",
      "appended a7_s7_t3!\n",
      "appended a7_s7_t4!\n",
      "appended a7_s8_t1!\n",
      "appended a7_s8_t2!\n",
      "appended a7_s8_t3!\n",
      "appended a7_s8_t4!\n",
      "appended a8_s1_t1!\n",
      "appended a8_s1_t2!\n",
      "appended a8_s1_t3!\n",
      "appended a8_s2_t1!\n",
      "appended a8_s2_t2!\n",
      "appended a8_s2_t3!\n",
      "appended a8_s2_t4!\n",
      "appended a8_s3_t1!\n",
      "appended a8_s3_t2!\n",
      "appended a8_s3_t3!\n",
      "appended a8_s3_t4!\n",
      "appended a8_s4_t1!\n",
      "appended a8_s4_t2!\n",
      "appended a8_s4_t3!\n",
      "appended a8_s4_t4!\n",
      "appended a8_s5_t1!\n",
      "appended a8_s5_t2!\n",
      "appended a8_s5_t3!\n",
      "appended a8_s5_t4!\n",
      "appended a8_s6_t1!\n",
      "appended a8_s6_t2!\n",
      "appended a8_s6_t3!\n",
      "appended a8_s6_t4!\n",
      "appended a8_s7_t1!\n",
      "appended a8_s7_t2!\n",
      "appended a8_s7_t3!\n",
      "appended a8_s7_t4!\n",
      "appended a8_s8_t1!\n",
      "appended a8_s8_t2!\n",
      "appended a8_s8_t3!\n",
      "appended a8_s8_t4!\n",
      "appended a9_s1_t1!\n",
      "appended a9_s1_t2!\n",
      "appended a9_s1_t3!\n",
      "appended a9_s1_t4!\n",
      "appended a9_s2_t1!\n",
      "appended a9_s2_t2!\n",
      "appended a9_s2_t3!\n",
      "appended a9_s2_t4!\n",
      "appended a9_s3_t1!\n",
      "appended a9_s3_t2!\n",
      "appended a9_s3_t3!\n",
      "appended a9_s3_t4!\n",
      "appended a9_s4_t1!\n",
      "appended a9_s4_t2!\n",
      "appended a9_s4_t3!\n",
      "appended a9_s4_t4!\n",
      "appended a9_s5_t1!\n",
      "appended a9_s5_t2!\n",
      "appended a9_s5_t3!\n",
      "appended a9_s5_t4!\n",
      "appended a9_s6_t1!\n",
      "appended a9_s6_t2!\n",
      "appended a9_s6_t3!\n",
      "appended a9_s6_t4!\n",
      "appended a9_s7_t1!\n",
      "appended a9_s7_t2!\n",
      "appended a9_s7_t3!\n",
      "appended a9_s7_t4!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended a9_s8_t1!\n",
      "appended a9_s8_t2!\n",
      "appended a9_s8_t3!\n",
      "appended a9_s8_t4!\n"
     ]
    }
   ],
   "source": [
    "cols = [0, 1, 2, 3, 4, 5, 'cat']\n",
    "\n",
    "df = pd.DataFrame(columns=cols)\n",
    "filedir = './Inertial/'\n",
    "\n",
    "for file in os.listdir(filedir):\n",
    "    \n",
    "    if not file.startswith(\".\"):\n",
    "        # import matlab file into df with label\n",
    "        data = loadmat(filedir + file)\n",
    "        df1 = pd.DataFrame(data['d_iner'])\n",
    "\n",
    "        category = file.split('_')[0]\n",
    "        subcat = str(file.split('_')[0] + '_' + file.split('_')[1] + '_' + file.split('_')[2])\n",
    "        df1['cat'] = category\n",
    "\n",
    "        # append to original df\n",
    "        df = df.append(df1)\n",
    "\n",
    "        print('appended {}!'.format(subcat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 classes only\n",
    "actions = ['a1', 'a2', 'a3']\n",
    "\n",
    "# subsetting df\n",
    "df = df[df['cat'].isin(actions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df.iloc[:, 0:6]\n",
    "Y = df.cat\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, Y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "DO NOT RUN THIS CODE CHUNK!!!\n",
    "\n",
    "SVM tuning takes up a considerable amount of time.\n",
    "Load the SVM pickle (see below chunk) for prediction instead!\n",
    "\n",
    "'''\n",
    "\n",
    "# svc parameter tuning\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    \n",
    "    grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    \n",
    "    return grid_search.best_params_\n",
    "\n",
    "best_params = svc_param_selection(X_train, Y_train, nfolds=5)\n",
    "\n",
    "# best cost and gamma for svc\n",
    "best_cost = best_params['C']\n",
    "best_gam = best_params['gamma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fname = 'clf_iner_svc.pkl'\n",
    "\n",
    "# pickling classifier\n",
    "#with open(fname, 'wb') as handle:\n",
    "#    pickle.dump(clf_svc, handle)\n",
    "\n",
    "# loading pickle\n",
    "with open('clf_iner_svc.pkl', 'rb') as handle:\n",
    "    clf_svc = pickle.load(handle)\n",
    "\n",
    "# svc training\n",
    "# clf_svc = SVC(C=best_cost, gamma=best_gam)\n",
    "clf_svc.fit(X_train, Y_train)\n",
    "\n",
    "# svc prediction\n",
    "Y_pred_svc = clf_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         a1       0.57      0.72      0.63      1554\n",
      "         a2       0.67      0.65      0.66      1461\n",
      "         a3       0.70      0.55      0.62      1702\n",
      "\n",
      "avg / total       0.65      0.64      0.64      4717\n",
      "\n",
      "\n",
      "[[1121  199  234]\n",
      " [ 354  947  160]\n",
      " [ 503  262  937]]\n",
      "\n",
      "acc: 63.706%\n"
     ]
    }
   ],
   "source": [
    "# class report, cm and accuracy\n",
    "print(classification_report(Y_test, Y_pred_svc), end='\\n\\n')\n",
    "print(confusion_matrix(Y_test, Y_pred_svc), end='\\n\\n')\n",
    "print('acc: {0:.3f}%'.format(accuracy_score(Y_test, Y_pred_svc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/ggu/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from numpy.random import seed\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "# train\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "encoded_Y_train = encoder.transform(Y_train)\n",
    "\n",
    "# test\n",
    "encoder.fit(Y_test)\n",
    "encoded_Y_test = encoder.transform(Y_test)\n",
    "\n",
    "\n",
    "# convert integers to dummy variables, ie. one-hot encoding\n",
    "# train\n",
    "dummy_Y_train = np_utils.to_categorical(encoded_Y_train)\n",
    "\n",
    "# test\n",
    "dummy_Y_test = np_utils.to_categorical(encoded_Y_test)\n",
    "\n",
    "# convert X to np array\n",
    "X_train_np = np.reshape(X_train.values, (11004,6))\n",
    "X_test_np = np.reshape(X_test.values, (4717,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11004/11004 [==============================] - 2s 163us/step - loss: 1.3528 - acc: 0.4221\n",
      "Epoch 2/100\n",
      "11004/11004 [==============================] - 2s 140us/step - loss: 0.9826 - acc: 0.4731\n",
      "Epoch 3/100\n",
      "11004/11004 [==============================] - 1s 132us/step - loss: 0.9448 - acc: 0.5031\n",
      "Epoch 4/100\n",
      "11004/11004 [==============================] - 2s 136us/step - loss: 0.9343 - acc: 0.5088\n",
      "Epoch 5/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.9288 - acc: 0.5115\n",
      "Epoch 6/100\n",
      "11004/11004 [==============================] - 2s 142us/step - loss: 0.9212 - acc: 0.5184\n",
      "Epoch 7/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.9174 - acc: 0.5133\n",
      "Epoch 8/100\n",
      "11004/11004 [==============================] - 2s 174us/step - loss: 0.9112 - acc: 0.5235\n",
      "Epoch 9/100\n",
      "11004/11004 [==============================] - 2s 137us/step - loss: 0.8907 - acc: 0.5370\n",
      "Epoch 10/100\n",
      "11004/11004 [==============================] - 1s 133us/step - loss: 0.8710 - acc: 0.5503\n",
      "Epoch 11/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.8589 - acc: 0.5575\n",
      "Epoch 12/100\n",
      "11004/11004 [==============================] - 2s 138us/step - loss: 0.8460 - acc: 0.5707\n",
      "Epoch 13/100\n",
      "11004/11004 [==============================] - 2s 141us/step - loss: 0.8323 - acc: 0.5784\n",
      "Epoch 14/100\n",
      "11004/11004 [==============================] - 2s 138us/step - loss: 0.8377 - acc: 0.5752\n",
      "Epoch 15/100\n",
      "11004/11004 [==============================] - 2s 142us/step - loss: 0.8224 - acc: 0.5828\n",
      "Epoch 16/100\n",
      "11004/11004 [==============================] - 2s 137us/step - loss: 0.8081 - acc: 0.5887\n",
      "Epoch 17/100\n",
      "11004/11004 [==============================] - 1s 132us/step - loss: 0.8051 - acc: 0.5944\n",
      "Epoch 18/100\n",
      "11004/11004 [==============================] - 2s 136us/step - loss: 0.7971 - acc: 0.6032\n",
      "Epoch 19/100\n",
      "11004/11004 [==============================] - 1s 134us/step - loss: 0.7937 - acc: 0.6069\n",
      "Epoch 20/100\n",
      "11004/11004 [==============================] - 2s 138us/step - loss: 0.7874 - acc: 0.6076\n",
      "Epoch 21/100\n",
      "11004/11004 [==============================] - 1s 134us/step - loss: 0.7833 - acc: 0.6111\n",
      "Epoch 22/100\n",
      "11004/11004 [==============================] - 1s 132us/step - loss: 0.7890 - acc: 0.6093\n",
      "Epoch 23/100\n",
      "11004/11004 [==============================] - 1s 131us/step - loss: 0.7877 - acc: 0.6071\n",
      "Epoch 24/100\n",
      "11004/11004 [==============================] - 1s 134us/step - loss: 0.7833 - acc: 0.6112\n",
      "Epoch 25/100\n",
      "11004/11004 [==============================] - 1s 130us/step - loss: 0.7788 - acc: 0.6162\n",
      "Epoch 26/100\n",
      "11004/11004 [==============================] - 1s 133us/step - loss: 0.7748 - acc: 0.6202\n",
      "Epoch 27/100\n",
      "11004/11004 [==============================] - 2s 138us/step - loss: 0.7766 - acc: 0.6185\n",
      "Epoch 28/100\n",
      "11004/11004 [==============================] - 2s 146us/step - loss: 0.7751 - acc: 0.6118\n",
      "Epoch 29/100\n",
      "11004/11004 [==============================] - 2s 141us/step - loss: 0.7804 - acc: 0.6134\n",
      "Epoch 30/100\n",
      "11004/11004 [==============================] - 2s 138us/step - loss: 0.7754 - acc: 0.6190\n",
      "Epoch 31/100\n",
      "11004/11004 [==============================] - 2s 147us/step - loss: 0.7674 - acc: 0.6205\n",
      "Epoch 32/100\n",
      "11004/11004 [==============================] - 2s 139us/step - loss: 0.7763 - acc: 0.6174\n",
      "Epoch 33/100\n",
      "11004/11004 [==============================] - 2s 164us/step - loss: 0.7673 - acc: 0.6210\n",
      "Epoch 34/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.7596 - acc: 0.6245\n",
      "Epoch 35/100\n",
      "11004/11004 [==============================] - 2s 138us/step - loss: 0.7732 - acc: 0.6165\n",
      "Epoch 36/100\n",
      "11004/11004 [==============================] - 1s 134us/step - loss: 0.7671 - acc: 0.6258\n",
      "Epoch 37/100\n",
      "11004/11004 [==============================] - 1s 132us/step - loss: 0.7655 - acc: 0.6214\n",
      "Epoch 38/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.7630 - acc: 0.6233\n",
      "Epoch 39/100\n",
      "11004/11004 [==============================] - 2s 138us/step - loss: 0.7626 - acc: 0.6250\n",
      "Epoch 40/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.7664 - acc: 0.6194\n",
      "Epoch 41/100\n",
      "11004/11004 [==============================] - 1s 136us/step - loss: 0.7558 - acc: 0.6247\n",
      "Epoch 42/100\n",
      "11004/11004 [==============================] - 2s 142us/step - loss: 0.7637 - acc: 0.6225\n",
      "Epoch 43/100\n",
      "11004/11004 [==============================] - 1s 134us/step - loss: 0.7580 - acc: 0.6261\n",
      "Epoch 44/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.7488 - acc: 0.6341\n",
      "Epoch 45/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.7510 - acc: 0.6268\n",
      "Epoch 46/100\n",
      "11004/11004 [==============================] - 1s 134us/step - loss: 0.7479 - acc: 0.6330\n",
      "Epoch 47/100\n",
      "11004/11004 [==============================] - 1s 129us/step - loss: 0.7498 - acc: 0.6285\n",
      "Epoch 48/100\n",
      "11004/11004 [==============================] - 2s 144us/step - loss: 0.7463 - acc: 0.6339\n",
      "Epoch 49/100\n",
      "11004/11004 [==============================] - 2s 141us/step - loss: 0.7461 - acc: 0.6337\n",
      "Epoch 50/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.7464 - acc: 0.6324\n",
      "Epoch 51/100\n",
      "11004/11004 [==============================] - 2s 140us/step - loss: 0.7450 - acc: 0.6347\n",
      "Epoch 52/100\n",
      "11004/11004 [==============================] - 1s 133us/step - loss: 0.7453 - acc: 0.6345\n",
      "Epoch 53/100\n",
      "11004/11004 [==============================] - 1s 132us/step - loss: 0.7416 - acc: 0.6364\n",
      "Epoch 54/100\n",
      "11004/11004 [==============================] - 1s 131us/step - loss: 0.7384 - acc: 0.6372\n",
      "Epoch 55/100\n",
      "11004/11004 [==============================] - 2s 138us/step - loss: 0.7385 - acc: 0.6379\n",
      "Epoch 56/100\n",
      "11004/11004 [==============================] - 2s 145us/step - loss: 0.7389 - acc: 0.6406\n",
      "Epoch 57/100\n",
      "11004/11004 [==============================] - 1s 132us/step - loss: 0.7395 - acc: 0.6367\n",
      "Epoch 58/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.7352 - acc: 0.6409\n",
      "Epoch 59/100\n",
      "11004/11004 [==============================] - 2s 151us/step - loss: 0.7404 - acc: 0.6365\n",
      "Epoch 60/100\n",
      "11004/11004 [==============================] - 2s 151us/step - loss: 0.7337 - acc: 0.6410\n",
      "Epoch 61/100\n",
      "11004/11004 [==============================] - 2s 199us/step - loss: 0.7387 - acc: 0.6388\n",
      "Epoch 62/100\n",
      "11004/11004 [==============================] - 2s 161us/step - loss: 0.7367 - acc: 0.6414\n",
      "Epoch 63/100\n",
      "11004/11004 [==============================] - 2s 176us/step - loss: 0.7352 - acc: 0.6430\n",
      "Epoch 64/100\n",
      "11004/11004 [==============================] - 2s 191us/step - loss: 0.7350 - acc: 0.6426\n",
      "Epoch 65/100\n",
      "11004/11004 [==============================] - 2s 202us/step - loss: 0.7328 - acc: 0.6425\n",
      "Epoch 66/100\n",
      "11004/11004 [==============================] - 2s 219us/step - loss: 0.7295 - acc: 0.6414\n",
      "Epoch 67/100\n",
      "11004/11004 [==============================] - 2s 183us/step - loss: 0.7299 - acc: 0.6431\n",
      "Epoch 68/100\n",
      "11004/11004 [==============================] - 2s 142us/step - loss: 0.7386 - acc: 0.6428\n",
      "Epoch 69/100\n",
      "11004/11004 [==============================] - 2s 140us/step - loss: 0.7326 - acc: 0.6453\n",
      "Epoch 70/100\n",
      "11004/11004 [==============================] - 2s 143us/step - loss: 0.7267 - acc: 0.6436\n",
      "Epoch 71/100\n",
      "11004/11004 [==============================] - 2s 138us/step - loss: 0.7282 - acc: 0.6439\n",
      "Epoch 72/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.7274 - acc: 0.6468\n",
      "Epoch 73/100\n",
      "11004/11004 [==============================] - 2s 140us/step - loss: 0.7285 - acc: 0.6464\n",
      "Epoch 74/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.7320 - acc: 0.6459\n",
      "Epoch 75/100\n",
      "11004/11004 [==============================] - 2s 138us/step - loss: 0.7332 - acc: 0.6424\n",
      "Epoch 76/100\n",
      "11004/11004 [==============================] - 1s 133us/step - loss: 0.7287 - acc: 0.6442\n",
      "Epoch 77/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.7282 - acc: 0.6479\n",
      "Epoch 78/100\n",
      "11004/11004 [==============================] - 1s 134us/step - loss: 0.7248 - acc: 0.6491\n",
      "Epoch 79/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.7256 - acc: 0.6471\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11004/11004 [==============================] - 1s 130us/step - loss: 0.7282 - acc: 0.6492\n",
      "Epoch 81/100\n",
      "11004/11004 [==============================] - 1s 133us/step - loss: 0.7249 - acc: 0.6466\n",
      "Epoch 82/100\n",
      "11004/11004 [==============================] - 1s 135us/step - loss: 0.7227 - acc: 0.6473\n",
      "Epoch 83/100\n",
      "11004/11004 [==============================] - 1s 130us/step - loss: 0.7261 - acc: 0.6454\n",
      "Epoch 84/100\n",
      "11004/11004 [==============================] - 1s 130us/step - loss: 0.7251 - acc: 0.6460\n",
      "Epoch 85/100\n",
      "11004/11004 [==============================] - 1s 136us/step - loss: 0.7293 - acc: 0.6471\n",
      "Epoch 86/100\n",
      "11004/11004 [==============================] - 1s 132us/step - loss: 0.7236 - acc: 0.6497\n",
      "Epoch 87/100\n",
      "11004/11004 [==============================] - 1s 130us/step - loss: 0.7247 - acc: 0.6454\n",
      "Epoch 88/100\n",
      "11004/11004 [==============================] - 1s 131us/step - loss: 0.7209 - acc: 0.6496\n",
      "Epoch 89/100\n",
      "11004/11004 [==============================] - 2s 144us/step - loss: 0.7229 - acc: 0.6469\n",
      "Epoch 90/100\n",
      "11004/11004 [==============================] - 1s 134us/step - loss: 0.7262 - acc: 0.6458\n",
      "Epoch 91/100\n",
      "11004/11004 [==============================] - 1s 130us/step - loss: 0.7234 - acc: 0.6469\n",
      "Epoch 92/100\n",
      "11004/11004 [==============================] - 1s 132us/step - loss: 0.7244 - acc: 0.6479\n",
      "Epoch 93/100\n",
      "11004/11004 [==============================] - 1s 133us/step - loss: 0.7212 - acc: 0.6470\n",
      "Epoch 94/100\n",
      "11004/11004 [==============================] - 1s 132us/step - loss: 0.7209 - acc: 0.6475\n",
      "Epoch 95/100\n",
      "11004/11004 [==============================] - 1s 129us/step - loss: 0.7227 - acc: 0.6481\n",
      "Epoch 96/100\n",
      "11004/11004 [==============================] - 1s 133us/step - loss: 0.7210 - acc: 0.6479\n",
      "Epoch 97/100\n",
      "11004/11004 [==============================] - 1s 136us/step - loss: 0.7220 - acc: 0.6483\n",
      "Epoch 98/100\n",
      "11004/11004 [==============================] - 1s 132us/step - loss: 0.7224 - acc: 0.6477\n",
      "Epoch 99/100\n",
      "11004/11004 [==============================] - 1s 132us/step - loss: 0.7203 - acc: 0.6489\n",
      "Epoch 100/100\n",
      "11004/11004 [==============================] - 1s 131us/step - loss: 0.7215 - acc: 0.6491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x119e45470>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed(123)\n",
    "\n",
    "# building the model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=6, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.005), metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train_np, dummy_Y_train, epochs=100, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "dummy_Y_pred = model.predict(X_test_np)\n",
    "\n",
    "Y_test_class = np.argmax(dummy_Y_test, axis=1)\n",
    "Y_pred_class = np.argmax(dummy_Y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4717/4717 [==============================] - 0s 24us/step\n",
      "\n",
      "acc: 63.98%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.76      0.65      1554\n",
      "          1       0.73      0.57      0.64      1461\n",
      "          2       0.68      0.60      0.64      1702\n",
      "\n",
      "avg / total       0.66      0.64      0.64      4717\n",
      "\n",
      "\n",
      "[[1176  176  202]\n",
      " [ 365  828  268]\n",
      " [ 550  138 1014]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "scores = model.evaluate(X_test_np, dummy_Y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(classification_report(Y_test_class, Y_pred_class), end='\\n\\n')\n",
    "print(confusion_matrix(Y_test_class, Y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import warnings\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inertial_data():\n",
    "    dir_path = \"Inertial\"\n",
    "    files = os.listdir(dir_path)\n",
    "    all_data = [] \n",
    "    for ii, file in enumerate(files, 1):\n",
    "        #print(ii, file)\n",
    "        if file.endswith(\".mat\"):\n",
    "            mat_contents = loadmat(dir_path+'/'+file)\n",
    "            d_skel=mat_contents[\"d_iner\"]\n",
    "            action = file.split(\"_\")[0]\n",
    "            all_data.append((d_skel,action))\n",
    "    print(len(all_data))\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(all_data, all_actions,split_at):\n",
    "    ddct = defaultdict(list)\n",
    "    training_data=[]\n",
    "    testing_data=[]\n",
    "    for X,y in all_data:\n",
    "        for action in all_actions:\n",
    "            if(action == y):\n",
    "                ddct[action].append(1)\n",
    "                if len(ddct[action]) < split_at+1:\n",
    "                    training_data.append((X,y))\n",
    "                else:\n",
    "                    testing_data.append((X,y))\n",
    "    return training_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9', 'a10', 'a11', 'a12', 'a13', 'a14', 'a15', 'a16', 'a17', 'a18', 'a19', 'a20', 'a21', 'a22', 'a23', 'a24', 'a25', 'a26', 'a27']\n"
     ]
    }
   ],
   "source": [
    "all_actions=[\"a\"+str(num) for num in range(1,28)]\n",
    "print(all_actions)\n",
    "\n",
    "split_at = 26 #as each data contains 32 records so 26 for training and 6 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "861\n"
     ]
    }
   ],
   "source": [
    "all_data_iner = load_inertial_data()\n",
    "training_data_iner, testing_data_iner = split_data(all_data_iner,all_actions,26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.. 702\n",
      "testing.. 159\n"
     ]
    }
   ],
   "source": [
    "print(\"training..\" , len(training_data_iner))\n",
    "print(\"testing..\" , len(testing_data_iner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_training_data_by_action(records,action):\n",
    "    action_pairs=[]\n",
    "    for X,y in records:\n",
    "        if y == action:\n",
    "            action_pairs.append((X,y))\n",
    "    return action_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_an_action(action, num_hidden_states, features,lengths):\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  \n",
    "    model = GaussianHMM(n_components=num_hidden_states, n_iter=1000,random_state=123,params=\"ct\").fit(features,lengths)\n",
    "    logL = model.score(features,lengths)\n",
    "    return model, logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hmm_formatted_features_inertial(records,action):\n",
    "    x_contatinated = np.zeros((1,6))\n",
    "    lengths=[]\n",
    "    \n",
    "    actions = fetch_training_data_by_action(records,action)\n",
    "    action_features,_ = zip(*actions)\n",
    "    print(len(action_features))\n",
    "    print(action_features[0].shape)\n",
    "    \n",
    "    \n",
    "    for subject_action in list(action_features):\n",
    "        lengths.append(subject_action.shape[0])\n",
    "        x_contatinated = np.append(x_contatinated,subject_action,axis=0)\n",
    "#         x_contatinated.append(subject_action)\n",
    "#         for i in range(subject_action.shape[1]):\n",
    "#             x_contatinated.append(subject_action) #subject_action[:,:,i].reshape(-1,))\n",
    "    #len(x_contatinated)\n",
    "    print(x_contatinated[0])\n",
    "    print(x_contatinated[1])\n",
    "    x_contatinated = np.delete(x_contatinated, 0, axis=0)\n",
    "    print(np.array(x_contatinated).shape)\n",
    "    print(x_contatinated[0])\n",
    "    print(x_contatinated[1])\n",
    "    print(lengths)\n",
    "    return np.array(x_contatinated),lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "(160, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-0.959473 -0.177734 -0.192871  5.221374  1.526718  0.152672]\n",
      "(4150, 6)\n",
      "[-0.959473 -0.177734 -0.192871  5.221374  1.526718  0.152672]\n",
      "[-0.961914 -0.15332  -0.159912  6.778626  1.954198  0.244275]\n",
      "[160, 154, 165, 158, 142, 156, 143, 157, 146, 127, 125, 150, 145, 143, 142, 160, 186, 170, 190, 180, 187, 180, 183, 189, 149, 163]\n"
     ]
    }
   ],
   "source": [
    "iner_X,iner_lengths = get_hmm_formatted_features_inertial(training_data_iner,'a1')\n",
    "iner_model, iner_logL = train_an_action('a1', 3, iner_X ,iner_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states trained in model for a1 is 3\n",
      "logL = -74799.60789966404\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of states trained in model for {} is {}\".format('a1', iner_model.n_components))\n",
    "print(\"logL = {}\".format(iner_logL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states trained in model for a1 is 3\n",
      "hidden state #0\n",
      "mean =  [ -0.66176301  -0.12486223  -0.2144198   12.32412024  -3.64200058\n",
      " -17.51877356]\n",
      "variance =  [  2.18639484e-01   1.56473128e-01   1.54231270e-01   1.02009770e+03\n",
      "   2.96687517e+02   5.63874099e+02]\n",
      "\n",
      "hidden state #1\n",
      "mean =  [  -0.58335413   -0.4446879    -0.3819892   -68.34798571   70.56537969\n",
      " -186.95872567]\n",
      "variance =  [  2.39588621e-01   5.24661217e-01   1.45156780e-01   6.81527344e+03\n",
      "   1.29299757e+04   7.21827994e+03]\n",
      "\n",
      "hidden state #2\n",
      "mean =  [  -0.85317914   -0.44072517   -0.26772264  159.4666085   -67.62036205\n",
      "  126.2903555 ]\n",
      "variance =  [  2.03241365e-01   1.92027037e-01   1.39350466e-01   6.03629137e+03\n",
      "   7.03057508e+03   9.11164864e+03]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_iner_model_stats(word, model):\n",
    "    print(\"Number of states trained in model for {} is {}\".format(word, iner_model.n_components))    \n",
    "    variance=np.array([np.diag(iner_model.covars_[i]) for i in range(iner_model.n_components)])    \n",
    "    for i in range(iner_model.n_components):  # for each hidden state\n",
    "        print(\"hidden state #{}\".format(i))\n",
    "        print(\"mean = \", iner_model.means_[i])\n",
    "        print(\"variance = \", variance[i])\n",
    "        print()\n",
    "    \n",
    "show_iner_model_stats('a1', iner_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "(160, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-0.959473 -0.177734 -0.192871  5.221374  1.526718  0.152672]\n",
      "(4150, 6)\n",
      "[-0.959473 -0.177734 -0.192871  5.221374  1.526718  0.152672]\n",
      "[-0.961914 -0.15332  -0.159912  6.778626  1.954198  0.244275]\n",
      "[160, 154, 165, 158, 142, 156, 143, 157, 146, 127, 125, 150, 145, 143, 142, 160, 186, 170, 190, 180, 187, 180, 183, 189, 149, 163]\n",
      "26\n",
      "(166, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[ -0.931152  -0.307861  -0.08252   23.603053  11.022901   5.618321]\n",
      "(4122, 6)\n",
      "[ -0.931152  -0.307861  -0.08252   23.603053  11.022901   5.618321]\n",
      "[ -0.92334   -0.361328  -0.074707  25.312977   8.732824   6.167939]\n",
      "[166, 146, 139, 146, 165, 131, 152, 159, 154, 140, 164, 139, 128, 147, 144, 168, 157, 178, 182, 186, 187, 190, 195, 194, 125, 140]\n",
      "26\n",
      "(184, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[ -0.995605  -0.418213  -0.188477  13.435115  15.267176  -7.236641]\n",
      "(4637, 6)\n",
      "[ -0.995605  -0.418213  -0.188477  13.435115  15.267176  -7.236641]\n",
      "[ -1.016113  -0.396729  -0.229492  18.167939  26.503817 -12.305344]\n",
      "[184, 188, 188, 177, 168, 158, 170, 174, 175, 169, 160, 189, 154, 179, 167, 178, 164, 206, 191, 204, 209, 193, 206, 188, 147, 151]\n"
     ]
    }
   ],
   "source": [
    "all_sequences={}\n",
    "all_lengths={}\n",
    "actions= ['a1','a2','a3']\n",
    "\n",
    "for action in actions:\n",
    "    X,lengths = get_hmm_formatted_features_inertial(training_data_iner,action)\n",
    "    all_sequences[action] = X\n",
    "    all_lengths[action]=lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import timeit\n",
    "# autoreload for automatically reloading changes made in model_selectors\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_actions(actions):\n",
    "    models={}\n",
    "    for action in actions:\n",
    "        print(\"training for \",action)\n",
    "        X = all_sequences[action]\n",
    "        print(X.shape)\n",
    "        lengths = all_lengths[action]\n",
    "        model, logL = train_an_action(action, 20, X ,lengths)\n",
    "        models[action]=model\n",
    "        #time.sleep(2)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for  a1\n",
      "(4150, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:460: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_),\n",
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:469: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_),\n",
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:624: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for  a2\n",
      "(4122, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:460: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_),\n",
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:469: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_),\n",
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:624: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for  a3\n",
      "(4637, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:460: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_),\n",
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:469: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_),\n",
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:624: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_),\n"
     ]
    }
   ],
   "source": [
    "models = train_all_actions(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on known actions\n",
    "def recognize_actions(features):\n",
    "    probabilities = []\n",
    "    guesses = []\n",
    "\n",
    "\n",
    "    print('Started recognizing ...')\n",
    "    \n",
    "    bestLL = float(\"-inf\")\n",
    "    bestAction = None\n",
    "    probs = {}\n",
    "    for action, model in models.items():\n",
    "        try:\n",
    "            ll = model.score(features)\n",
    "            if ll > bestLL:\n",
    "                    bestLL = ll\n",
    "                    bestAction = action\n",
    "                    probs[action] = ll\n",
    "        except:\n",
    "            print(\" ! \",end=\" \")\n",
    "            pass\n",
    "    \n",
    "    guesses.append(bestAction)\n",
    "    probabilities.append(probs)\n",
    "    return guesses,probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(143, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-0.996094 -0.179443  0.029297  9.068702  3.053435 -0.549618]\n",
      "(1000, 6)\n",
      "[-0.996094 -0.179443  0.029297  9.068702  3.053435 -0.549618]\n",
      "[ -1.00415000e+00  -1.68701000e-01   1.19630000e-02   1.20000000e+01\n",
      "   4.09160300e+00  -7.32824000e-01]\n",
      "[143, 141, 197, 169, 181, 169]\n",
      "2\n",
      "-20.4220465369\n",
      "12\n",
      "-19.5314564208\n",
      "0\n",
      "-20.0523415719\n",
      "[1 1]\n",
      "[6 6]\n",
      "[0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:460: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_),\n",
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:452: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_), framelogprob)\n"
     ]
    }
   ],
   "source": [
    "test_AsX,test_A3sL = get_hmm_formatted_features_inertial(testing_data_iner,'a1')\n",
    "pr1 =models['a1'].score([test_AsX[0],test_AsX[1]])\n",
    "pr2 =models['a2'].predict([test_AsX[0],test_AsX[1]])\n",
    "pr3 =models['a3'].predict([test_AsX[0],test_AsX[1]])\n",
    "print(sum(models['a1'].predict([test_AsX[0],test_AsX[1]])))\n",
    "print(pr1)\n",
    "print(sum(pr2))\n",
    "print(models['a2'].score([test_AsX[0],test_AsX[1]]))\n",
    "print(sum(pr3))\n",
    "print(models['a3'].score([test_AsX[0],test_AsX[1]]))\n",
    "print(models['a1'].predict([test_AsX[0],test_AsX[1]]))\n",
    "print(models['a2'].predict([test_AsX[0],test_AsX[1]]))\n",
    "print(models['a3'].predict([test_AsX[0],test_AsX[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(143, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-0.996094 -0.179443  0.029297  9.068702  3.053435 -0.549618]\n",
      "(1000, 6)\n",
      "[-0.996094 -0.179443  0.029297  9.068702  3.053435 -0.549618]\n",
      "[ -1.00415000e+00  -1.68701000e-01   1.19630000e-02   1.20000000e+01\n",
      "   4.09160300e+00  -7.32824000e-01]\n",
      "[143, 141, 197, 169, 181, 169]\n",
      "Started recognizing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:460: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['a2'], [{'a1': -20.422046536923393, 'a2': -19.531456420806542}])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_feature = get_hmm_formatted_features_inertial(testing_data_iner,'a1')\n",
    "test_AsX,test_A3sL = sample_feature\n",
    "recognize_actions([test_AsX[0],test_AsX[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(143, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-0.996094 -0.179443  0.029297  9.068702  3.053435 -0.549618]\n",
      "(143, 6)\n",
      "[-0.996094 -0.179443  0.029297  9.068702  3.053435 -0.549618]\n",
      "[ -1.00415000e+00  -1.68701000e-01   1.19630000e-02   1.20000000e+01\n",
      "   4.09160300e+00  -7.32824000e-01]\n",
      "[143]\n",
      "Started recognizing ...\n",
      "1\n",
      "(141, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[ -0.988281  -0.173096   0.023682  13.679389   2.473282   1.954198]\n",
      "(141, 6)\n",
      "[ -0.988281  -0.173096   0.023682  13.679389   2.473282   1.954198]\n",
      "[ -0.995117  -0.161621   0.        14.381679   3.389313   3.206107]\n",
      "[141]\n",
      "Started recognizing ...\n",
      "1\n",
      "(197, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-0.994629 -0.295654  0.154297 -2.687023  5.40458   1.374046]\n",
      "(197, 6)\n",
      "[-0.994629 -0.295654  0.154297 -2.687023  5.40458   1.374046]\n",
      "[-1.       -0.280762  0.101807 -3.51145   5.832061  1.374046]\n",
      "[197]\n",
      "Started recognizing ...\n",
      "1\n",
      "(169, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-1.021484 -0.202637  0.039795 -2.290076  3.755725  1.496183]\n",
      "(169, 6)\n",
      "[-1.021484 -0.202637  0.039795 -2.290076  3.755725  1.496183]\n",
      "[-1.0271   -0.203125 -0.0271    0.091603  6.381679  2.167939]\n",
      "[169]\n",
      "Started recognizing ...\n",
      "1\n",
      "(181, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-1.008301 -0.198242  0.115967  1.89313   2.80916  -0.030534]\n",
      "(181, 6)\n",
      "[-1.008301 -0.198242  0.115967  1.89313   2.80916  -0.030534]\n",
      "[-1.011963 -0.203369  0.101318  1.374046  3.633588 -0.21374 ]\n",
      "[181]\n",
      "Started recognizing ...\n",
      "1\n",
      "(169, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-1.005615 -0.176758  0.176025  3.603053 -0.946565  0.519084]\n",
      "(169, 6)\n",
      "[-1.005615 -0.176758  0.176025  3.603053 -0.946565  0.519084]\n",
      "[-1.000977 -0.170166  0.156006  5.862595 -1.343511 -0.183206]\n",
      "[169]\n",
      "Started recognizing ...\n",
      "1\n",
      "(134, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[ -0.972412  -0.350098  -0.118652 -52.305344 -11.114504   4.671756]\n",
      "(134, 6)\n",
      "[ -0.972412  -0.350098  -0.118652 -52.305344 -11.114504   4.671756]\n",
      "[ -0.969482  -0.360596  -0.127686 -75.267176 -14.534351   8.48855 ]\n",
      "[134]\n",
      "Started recognizing ...\n",
      "1\n",
      "(133, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[ -0.975586  -0.206055  -0.091553 -15.938931  -3.51145    1.648855]\n",
      "(133, 6)\n",
      "[ -0.975586  -0.206055  -0.091553 -15.938931  -3.51145    1.648855]\n",
      "[ -0.977783  -0.218994  -0.09082  -21.312977  -4.824427   3.114504]\n",
      "[133]\n",
      "Started recognizing ...\n",
      "1\n",
      "(164, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[ -1.037842  -0.319092  -0.040527 -16.946565   6.229008  -6.076336]\n",
      "(164, 6)\n",
      "[ -1.037842  -0.319092  -0.040527 -16.946565   6.229008  -6.076336]\n",
      "[ -1.029785  -0.321045  -0.041016 -15.572519  10.015267  -9.251908]\n",
      "[164]\n",
      "Started recognizing ...\n",
      "1\n",
      "(144, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[ -1.041504  -0.350098  -0.049561 -36.824427  -0.21374  -14.992366]\n",
      "(144, 6)\n",
      "[ -1.041504  -0.350098  -0.049561 -36.824427  -0.21374  -14.992366]\n",
      "[ -1.021973  -0.403564  -0.040771 -23.114504   6.687023 -20.      ]\n",
      "[144]\n",
      "Started recognizing ...\n",
      "1\n",
      "(142, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[ -1.007568  -0.201172   0.092285 -10.381679   0.580153  -3.541985]\n",
      "(142, 6)\n",
      "[ -1.007568  -0.201172   0.092285 -10.381679   0.580153  -3.541985]\n",
      "[ -1.020752  -0.264893   0.073486 -14.900763   0.244275  -4.732824]\n",
      "[142]\n",
      "Started recognizing ...\n",
      "1\n",
      "(145, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[ -1.021729  -0.299805   0.07373  -17.832061  -0.946565  -4.335878]\n",
      "(145, 6)\n",
      "[ -1.021729  -0.299805   0.07373  -17.832061  -0.946565  -4.335878]\n",
      "[ -1.02270500e+00  -3.50098000e-01  -1.95310000e-02  -3.46870230e+01\n",
      "  -2.47328200e+00  -7.14503800e+00]\n",
      "[145]\n",
      "Started recognizing ...\n",
      "1\n",
      "(151, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[ -1.001221  -0.180176  -0.257568 -12.70229   11.541985  -6.656489]\n",
      "(151, 6)\n",
      "[ -1.001221  -0.180176  -0.257568 -12.70229   11.541985  -6.656489]\n",
      "[ -1.040771  -0.160889  -0.426025  18.198473  22.137405  -7.358779]\n",
      "[151]\n",
      "Started recognizing ...\n",
      "1\n",
      "(156, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-0.992432 -0.208008 -0.072021  2.137405  2.076336  3.572519]\n",
      "(156, 6)\n",
      "[-0.992432 -0.208008 -0.072021  2.137405  2.076336  3.572519]\n",
      "[-0.993408 -0.164551 -0.12207   3.969466  3.51145   3.419847]\n",
      "[156]\n",
      "Started recognizing ...\n",
      "1\n",
      "(165, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-1.004395 -0.195801  0.080322 -2.076336  1.618321  0.244275]\n",
      "(165, 6)\n",
      "[-1.004395 -0.195801  0.080322 -2.076336  1.618321  0.244275]\n",
      "[-1.007568 -0.193848  0.058838 -2.290076  2.167939  0.366412]\n",
      "[165]\n",
      "Started recognizing ...\n",
      "1\n",
      "(151, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-1.012451 -0.214844  0.072998 -4.183206  0.335878  1.862595]\n",
      "(151, 6)\n",
      "[-1.012451 -0.214844  0.072998 -4.183206  0.335878  1.862595]\n",
      "[ -1.05175800e+00  -2.73193000e-01   7.32000000e-04  -5.70992400e+00\n",
      "  -2.19847300e+00   4.48855000e+00]\n",
      "[151]\n",
      "Started recognizing ...\n",
      "1\n",
      "(158, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-1.00415  -0.174805  0.115723 -3.816794  1.282443  0.122137]\n",
      "(158, 6)\n",
      "[-1.00415  -0.174805  0.115723 -3.816794  1.282443  0.122137]\n",
      "[-1.006836 -0.182617  0.110107 -4.335878  1.89313   0.458015]\n",
      "[158]\n",
      "Started recognizing ...\n",
      "1\n",
      "(169, 6)\n",
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[-0.999023 -0.196045  0.09082   2.992366  1.19084  -0.091603]\n",
      "(169, 6)\n",
      "[-0.999023 -0.196045  0.09082   2.992366  1.19084  -0.091603]\n",
      "[-1.012695 -0.187012  0.07251  -0.671756  1.099237 -0.21374 ]\n",
      "[169]\n",
      "Started recognizing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ggu/anaconda/lib/python3.6/site-packages/hmmlearn/base.py:460: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(self.transmat_),\n"
     ]
    }
   ],
   "source": [
    "test_actions=[\"a1\",\"a2\",\"a3\"]\n",
    "predicted_iner=[]\n",
    "actual_iner=[]\n",
    "\n",
    "for Xs,ys in [(Xs,ys) for Xs,ys in testing_data_iner if ys in test_actions]:\n",
    "    X,L = get_hmm_formatted_features_inertial([(Xs,ys)],ys)\n",
    "    predicted_iner.append(recognize_actions(X)[0][0])\n",
    "    actual_iner.append(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.944444444444\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         a1       1.00      0.83      0.91         6\n",
      "         a2       0.86      1.00      0.92         6\n",
      "         a3       1.00      1.00      1.00         6\n",
      "\n",
      "avg / total       0.95      0.94      0.94        18\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "[[5 1 0]\n",
      " [0 6 0]\n",
      " [0 0 6]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAEmCAYAAAATPUntAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/R++KpYrrvJfBQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1057c0438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(accuracy_score(actual_iner, predicted_iner))\n",
    "print(classification_report(actual_iner, predicted_iner, target_names=test_actions))\n",
    "cnf_matrix_iner = confusion_matrix(actual_iner, predicted_iner)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_iner, classes=test_actions,\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
