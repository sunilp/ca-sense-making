{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QcL9prf_EqPx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center>\n",
    "<h1> Human  Activity Recognition </h1>\n",
    "\n",
    "<h3> Sense Making And Insight Discovery</h3>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<small>\n",
    "<ul>\n",
    "<li> ANAND RAJAN <small>A0163200B</small>     </li>\n",
    "<li> GU JIAHAO GAELAN <small>A0163252M</small> </li>\n",
    "<li> SUMA MULPURU <small>A0163231U</small>     </li>\n",
    "<li> SUNIL PRAKASH <small>A0163449X</small>\t   </li>\n",
    "</ul>\n",
    "</small>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rk2oS_khE26I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z10Ws0JCGYjw",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "\n",
    "1.   Objective\n",
    "2.   Problem Description\n",
    "3.   Data Set\n",
    "4.   Literature Survey\n",
    "5.   Technological Approach and Innovation\n",
    "6.   Experimental Results\n",
    "7.   Performance Evaluation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9aW8f24zIAi6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UgOSPo1GIJJP"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "*   To make sense of data from modality sensors for robust human action recognition\n",
    "*   To develop an algorithm to classify different human activities into categories using \n",
    "    data from single sensor or from fusion of multiple sensors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Y09mtchIqOX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xsabWsBrIzyL",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pErZIDASJVES"
   },
   "source": [
    "\n",
    "- Human activity detection has several applications in many fields\n",
    "- There are several different kinds of sensors available to address this task\n",
    "- The use of multi modal sensors for human activity detection has increased these days\n",
    "<img src=\"images/Image1.jpg\" style=\"float:right;z-index:-99\"/>\n",
    "\n",
    "- The data available is from four different modalities\n",
    "- The human activities are to be classified using either one or all of the sensors data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBeEmwUWJveN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset Explanation\n",
    "\n",
    "- The available data set has four temporarily synchronized data modalities\n",
    "<img src=\"images/Image2.jpg\" style=\"height:400px;float:right;z-index:-99\"/>\n",
    "- The modalities include:\n",
    "    - RGB Videos\n",
    "    - Depth Data\n",
    "    - Skeletal Positions\n",
    "    - Inertial Signals\n",
    "- The data set is obtained from wearable sensors and Kinect\n",
    "- Data is a comprehensive set of **27** human actions performed by 8 humans (4 males and  4 females)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HCxpqSyKOvt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZP8QbDrJvH2"
   },
   "source": [
    "\n",
    "<small>\n",
    "- Human Activity Recognition is a hot research topic as it has different applications in many fields\n",
    "- Human activity is complex and dynamic and hence the algorithms should model nuances in human activities\n",
    "- Each activity may have in turn several sub activities; eg. The activity of brushing teeth has sub activities of squeezing out the paste, bringing brush close to face etc.\n",
    "- Hands play an important role in carrying out human activities\n",
    "- Motion information is also important in classifying a human’s activities\n",
    "- This task is accomplished by using different sensors like wearable sensors, vision based devices etc\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtaI3B4eLNUY",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<small>\n",
    "- Wearable technology is a category of devices (sensors) that can be worn by the consumer and often include tracking information related to health and fitness\n",
    "- They have diagnostic as well as monitoring applications\n",
    "- Wearable inertial tracking is well accepted due to its convenience for free style motion tracking with high accuracy\n",
    "- Wearable inertial tracking allows for unlimited estimation of limb orientations under fast motions which could improve the performance of motion capture evidently \n",
    "- The wearable inertial sensor used here is the low cost wireless inertial sensor\n",
    "- Human soft tissue artifact is a main source of errors no matter the wearable is placed on the clothing or directly on the skin\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30XR1yASL-1z",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<small>\n",
    "- Kinect is an unobtrusive motion sensor device which is reliable, of competitive cost, powerful, easy to set up, use and accessible\n",
    "<img src=\"images/Image3.jpg\" style=\"float:right;z-index:-99\"/>\n",
    "\n",
    "- With the use of depth  sensors like Kinect human activity recognition system using depth maps can be developed\n",
    "- Kinect sensors are a good source of information as they are not effected by environmental light variations\n",
    "- This device facilitates extraction of 3D points on human joints\n",
    "- Action recognition has a wide range of application areas such as computer vision, robotics, machine learning, ambient intelligence, medical and many more commercial uses\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUSNy9KdMR_p",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<small>\n",
    "- It is used for retrieving 3D information of a scene analyzing the depth map and skeletal joint information of the human body\n",
    "<img src=\"images/Image4.jpg\" style=\"float:right;z-index:-99\"/>\n",
    "- This helps the Kinect sensor to identify the type of action being performed by the person such as standing, walking, punching, sitting, waving etc.\n",
    "- Using the 3D joint information, the Kinect identifies the gestures and actions being performed by the human body and then the machine responds according to the action input\n",
    "- If the human body is taken as a model set of joints connecting the relevant body parts, then the most significant configurations of its positions are used to define recurrent postures\n",
    "- The activity recognition method should be able to guarantee an acceptable accuracy, real time processing, low power consumption which Kinect provides\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IebHVIWoMZAw",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<small>\n",
    "- Skeleton streams are the most important features of a Kinect. \n",
    "- Its provide the position and location of the persons whether they tracked or not.\n",
    "- The skeletons which are not tracked are given to zero value returned.\n",
    "- Kinect follows the below steps to perform image acquisition:\n",
    "\n",
    "    - Colour image frames extraction: Out of various resolutions available we have to obtained only 640x480 resolution images\n",
    "    - Depth image frame extraction: It is also obtained at the resolutions of 640x480 images\n",
    "    - Skeleton data used to track and extract the activity of user\n",
    "    - Background subtraction \n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SeGAFzTVLjM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Technology Approach and Innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "faGEKBxpVWgN"
   },
   "source": [
    "\n",
    "**Hidden Markov Model (HMM)**\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Hmm_temporal_bayesian_net.svg/500px-Hmm_temporal_bayesian_net.svg.png\" style=\"float:right;z-index:-99\"/>\n",
    "\n",
    "<small>\n",
    "- As Sequential data is different from normal snapshot data, composition of all sequences results to a particular action.\n",
    "\n",
    "- Hidden Markov Models provide a more powerful framework as they allow the\n",
    "states to be separated from the input without requiring a direct mapping.\n",
    "- In HMM, the states are hidden and abstract while the inputs are observables\n",
    "\n",
    "- The underlying stochastic process is not directly observable but can be observed\n",
    "only through another set of stochastic processes that produce the sequence of\n",
    "observations\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Initial State\n",
    "- Hidden States with transition probability\n",
    "<img src=\"Images/Image11.png\" style=\"height:200px;float:right;z-index:-99\"/>\n",
    "- Inner loop for stating in same state for longer period of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prp2etnjNaaw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification - Individual Sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uesbLn0FNmpN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skeleton Sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAXY8FHZQ5BN"
   },
   "source": [
    "\n",
    "- 20 joints positions in the world coordinate (i.e., x, y, and z) were recorded and the screen coordinates were mapped to the depth images with > 50 sequences of data\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sunilp/ca-open-resources/master/images/a1_skeleton.gif\" style=\"width:250px;height:350px;float:right\"/>\n",
    "\n",
    "\n",
    "\n",
    "#### Activities Trained\n",
    "\n",
    "1. *a1* - right arm swipe to the left,\n",
    "2. *a2* - right arm swipe to the right,\n",
    "3. *a3* - right hand wave\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature Extraction\n",
    "\n",
    "<img src=\"Images/Image12.png\" style=\"width:450px;float:right\"/>\n",
    "\n",
    "- delta X - change in x direction \n",
    "- delta Y - change in y direction \n",
    "- delta Z - change in z direction \n",
    "- 4 Possible Hidden States\n",
    "- Each with 3 * 20 features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 202,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 703,
     "status": "error",
     "timestamp": 1509876519625,
     "user": {
      "displayName": "Sunil Prakash",
      "photoUrl": "//lh6.googleusercontent.com/-NCCjmb_FW_4/AAAAAAAAAAI/AAAAAAAAFdA/kDJQZbDwYXI/s50-c-k-no/photo.jpg",
      "userId": "102916193033040622145"
     },
     "user_tz": -480
    },
    "id": "5-z8RpYvEqPz",
    "outputId": "f48273c3-c76c-44ae-af59-0d7082f00aaf",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from SensorHMM import SensorHMM\n",
    "%matplotlib inline\n",
    "\n",
    "actions=[\"a1\",\"a2\",\"a3\",\"a4\",\"a5\"]#[\"a\"+str(num) for num in range(1,28)] #[\"a1\",\"a2\",\"a3\"]#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RuXunxFAEqP6",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "skeleton_model = SensorHMM(\"skeleton\",data_directory=\"Skeleton\", data_key=\"d_skel\",\n",
    "                           no_features=20,n_components=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "TO4Zcty8EqP8",
    "outputId": "0df57fbb-706b-4618-b689-dd54df05d3f2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "all_data_skeleton = skeleton_model.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "lR8b14raEqQA",
    "outputId": "f4318210-998d-4a74-af0d-0442f499e00c"
   },
   "outputs": [],
   "source": [
    "training_data, testing_data = skeleton_model.split_data(all_data_skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "FVTv4vrLEqQD",
    "outputId": "26798e56-439c-4205-af9f-b7ed255ace26"
   },
   "outputs": [],
   "source": [
    "skeleton_model.fit(training_data,actions=actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OXlPoVlZEqQG",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "predictedY,predictedP = skeleton_model.predict(testing_data)\n",
    "_,testY = zip(*[(testx,testy) for testx,testy in testing_data if testy in actions])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "NEvy6lwJEqQI",
    "outputId": "f20fb976-0e12-438e-a8c4-5a973c3f4b84",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "skeleton_model.print_classification_report(testY,predictedY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "action_dic={'a1':'Swipe Left', 'a2':'Swipe right','a3':'Wave', 'a4':'Clap', 'a5':'Throw'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def predict_unseen(action='a1'):\n",
    "    prediction,likelyhood=[None],[None]\n",
    "    try:\n",
    "        some_records = skeleton_model.fetch_training_data_by_action(testing_data,action)\n",
    "        prediction,likelyhood = skeleton_model.predict([some_records[0]],single=True)\n",
    "    except:\n",
    "        pass\n",
    "    print(\"Predicted Action \", prediction[0], action_dic.get(prediction[0]),\" ,with likelehood\",likelyhood[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4LnrQ6NPhzD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inertial Sensor\n",
    "\n",
    "* Inertial sensors usually provide information about **acceleration** and **rotation** in a 3D space\n",
    "\n",
    "* In this case, the sensor was worn on either the right wrist or right thigh, and 3-axis acceleration and rotation signal data was collected in one channel\n",
    "<img src=\"http://www.utdallas.edu/~cxc123730/UTD-MAD/sensor_placement.png\" style=\"float:right;height:260px\"/>\n",
    "* It is worthwhile to note that since this is a low-cost 9-axis MEMS sensor, the data may not be highly accurate\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEKQ23fZihXs",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cols = [0, 1, 2, 3, 4, 5, 'cat']\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "df_plot = pd.DataFrame(columns=cols)\n",
    "filedir = './Inertial/'\n",
    "\n",
    "for file in os.listdir(filedir):\n",
    "    \n",
    "    if file.endswith(\".mat\"):\n",
    "        # import matlab file into df with label\n",
    "        data = loadmat(filedir + file)\n",
    "        df_plot1 = pd.DataFrame(data['d_iner'])\n",
    "\n",
    "        subcat = str(file.split('_')[0] + '_' + file.split('_')[1] + '_' + file.split('_')[2])\n",
    "        df_plot1['cat'] = subcat\n",
    "\n",
    "        # append to original df\n",
    "        df_plot = df_plot.append(df_plot1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_plot_a1 = df_plot[df_plot.cat == 'a1_s1_t1']\n",
    "df_plot_a1.rename(columns={0: 'acc_X',\n",
    "                           1: 'acc_Y', \n",
    "                           2: 'acc_Z',\n",
    "                           3: 'gyro_X', \n",
    "                           4: 'gyro_Y', \n",
    "                           5: 'gyro_Z'}, inplace=True)\n",
    "\n",
    "None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot_a1.loc[:, 'acc_X':'acc_Z'].plot(title='Swipe Left Action - Acceleration')\n",
    "df_plot_a1.loc[:, 'gyro_X':'gyro_Z'].plot(title='Swipe Left Action - Gyroscope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 185,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 760,
     "status": "error",
     "timestamp": 1509875814829,
     "user": {
      "displayName": "Gaelan Gu",
      "photoUrl": "//lh4.googleusercontent.com/-Ih4vOElpsOo/AAAAAAAAAAI/AAAAAAAAHqk/kyvBZG5n4ng/s50-c-k-no/photo.jpg",
      "userId": "104129723920476939943"
     },
     "user_tz": -480
    },
    "id": "FvNzmlt2EqQM",
    "outputId": "1afda228-1650-46d3-d16c-5fbd0fa1cf30",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "inertial_model = SensorHMM(\"inertial\",data_directory=\"Inertial\", data_key=\"d_iner\",\n",
    "                           no_features=6,n_components=20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8000,
     "status": "error",
     "timestamp": 1509875797381,
     "user": {
      "displayName": "Gaelan Gu",
      "photoUrl": "//lh4.googleusercontent.com/-Ih4vOElpsOo/AAAAAAAAAAI/AAAAAAAAHqk/kyvBZG5n4ng/s50-c-k-no/photo.jpg",
      "userId": "104129723920476939943"
     },
     "user_tz": -480
    },
    "id": "x_Q-gzEPEqQP",
    "outputId": "1f1aa6f0-8655-4785-a29c-0e92c3566f80",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# data import\n",
    "all_data_inertial = inertial_model.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "hKJ_2Vp6EqQT",
    "outputId": "603952d0-0730-427a-bbc1-e2edd04a75bb"
   },
   "outputs": [],
   "source": [
    "# train-test split\n",
    "training_data_inertial, testing_data_inertial = inertial_model.split_data(all_data_inertial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4vrxaUVoZzub",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "We train the Hidden Markov Model on the inertial data for the first 3 actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {},
      {},
      {},
      {},
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "qr_NqByYEqQX",
    "outputId": "11021d83-1d4a-46ad-8cd5-8df78908492d",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "inertial_model.fit(training_data_inertial,actions=actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTJOhCylbQZz",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Classifying the Actions\n",
    "<small>\n",
    "Now we classify the actions using the trained inertial model on the test data of the 3 actions.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "2y_O6fPlEqQZ",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "predictedY_inertial,predictedP_inertial = inertial_model.predict(testing_data_inertial)\n",
    "_,testY_inertial = zip(*[(testx,testy) for testx,testy in testing_data_inertial if testy in actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "8FwtTHmWEqQc",
    "outputId": "fc37dabc-5413-4ffa-d3d6-556b04b05ac9",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "inertial_model.print_classification_report(testY_inertial,predictedY_inertial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCPgyIzhgw4_",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* A very favourable accuracy of 96% was achieved on the test data\n",
    "* Only one misclassification when the *swipe right* (`a2`) action was mistaken for a *wave* (`a3`)\n",
    "* This might be due to lesser data loss from the inertial sensor (as compared to the depth sensor) so the model is able to easily recognize and differentiate the actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def predict_unseen_iner(action='a1'):\n",
    "    prediction,likelyhood=[None],[None]\n",
    "    try:\n",
    "        some_records = inertial_model.fetch_training_data_by_action(testing_data,action)\n",
    "        prediction,likelyhood = skeleton_model.predict([some_records[0]],single=True)\n",
    "    except:\n",
    "        pass\n",
    "    print(\"Predicted Action \", prediction[0], \" ,with likelehood\",likelyhood[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDkFg9zOWY_V",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Depth Sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8pOjaZbRQDQ"
   },
   "source": [
    "<img src=\"Images/Image5.jpg\" style=\"float:right;height:260px\"/>\n",
    "\n",
    "\n",
    "<small>\n",
    "*  In the recent years, depth information is a particularly useful cue in human machine interface (HMI) applications.\n",
    "*   The depth map-based methods rely mainly on features, either local or global, extracted from the feature space.\n",
    "*   Compared to visual data, depth maps provide metric i.e measurements of the geometry that are invariant to lighting.\n",
    "-   On the other hand, the main disadvantage of this technique is that depth sequences may be sensitive to occlusions \n",
    "    and the textures of the images are not as good as that of colour images.\n",
    "-   At present, Gesture Recognition through visual and depth information is one of the main active research topics in the \n",
    "    computer vision community.  \n",
    "-  In this approach we used three techniques such as\n",
    "   *   Edge Detection using Sobel Feldman Algorithm\n",
    "   *   Convolution using CNN\n",
    "   *   Feature extraction\n",
    "   *   Classifcation using Hidden Markov Model\n",
    "</small>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPMHQp7hEqQf",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "DZDE0E4fEqQg",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "depth_model = SensorHMM(\"depth\",data_directory=\"Depth\", data_key=\"d_depth\",\n",
    "                           no_features=19,n_components=4,params=\"f\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "5RL3qyy1EqQj",
    "outputId": "448bde04-c6d5-4dbc-ac90-9e0be953aac0"
   },
   "outputs": [],
   "source": [
    "all_data_depth = depth_model.load_data()\n",
    "#depth_model.all_data=all_data_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "f_mLoLpDEqQn",
    "outputId": "d79f198b-5adf-4402-de5d-755a4f6bc525"
   },
   "outputs": [],
   "source": [
    "training_data_depth, testing_data_depth = depth_model.split_data(all_data_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KTibLNuLiyGb",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "k6qRpPWQEqQq",
    "outputId": "255693b6-1dca-4b94-de77-bd2e3df1b0b8",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "depth_model.fit(training_data_depth,actions=actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcM8HDE3i_0B",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Classifying the Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "fK5IirxHEqQs",
    "outputId": "4ebf6e58-2dc1-456e-cb00-f84964a949f7",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "predictedY_depth,predictedP_depth = depth_model.predict(testing_data_depth)\n",
    "_,testY_depth = zip(*[(testx,testy) for testx,testy in testing_data_depth if testy in actions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpxmUXwDjLd7",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "TZvflSLsEqQw",
    "outputId": "4a8e0052-edb9-4bad-cc52-aab6af620a86",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "depth_model.print_classification_report(testY_depth,predictedY_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeni88D0Pxjv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iXHPzC34Q8gU"
   },
   "source": [
    "- Each Sensors are classified individually\n",
    "<img src=\"https://www.frontiersin.org/files/Articles/160288/frobt-02-00028-HTML/image_m/frobt-02-00028-g012.jpg\" style=\"height:250px;float:right\"/>\n",
    "\n",
    "- Fusion model is created at end layer to fuse all likelyhoods\n",
    "- produces the best probability with low variance\n",
    "\n",
    "\n",
    "\n",
    "We applied late fusion, with Most **likelyhood** estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fuse_models(actions,testing_datas,models):\n",
    "    \n",
    "    predictions={}\n",
    "    votings=np.zeros(len(actions))\n",
    "    for idx,model in enumerate(models):\n",
    "        actions,_ = model.predict(testing_datas[idx])\n",
    "        predictions[model.name]=actions\n",
    "        \n",
    "    preds_voting_dict={}\n",
    "    for idx in range(len(predictions[\"skeleton\"])):\n",
    "        preds_voting_dict[idx]=np.copy(votings)\n",
    "            \n",
    "           \n",
    "    for idx, preds in predictions.items():\n",
    "        for pred_idx, pred in enumerate(preds):\n",
    "            index = int(pred[1:])-1\n",
    "            pred_voting = preds_voting_dict[pred_idx]\n",
    "            if idx=='inertial':\n",
    "                pred_voting[index]+=1.5\n",
    "            else:\n",
    "                pred_voting[index]+=1\n",
    "            \n",
    "    final_voting_result=[]\n",
    "    for idx, preds in preds_voting_dict.items():\n",
    "        final_voting_result.append('a'+ str(np.argmax(preds)+1))\n",
    "    \n",
    "    #print(predictions)\n",
    "    #print(preds_voting_dict)\n",
    "    return final_voting_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "testing_data_skel_f,testing_data_iner_f,testing_data_dept_f=[],[],[]\n",
    "for action in actions:\n",
    "    testing_data_skel_f += skeleton_model.fetch_training_data_by_action(testing_data,action)\n",
    "    testing_data_iner_f += inertial_model.fetch_training_data_by_action(testing_data_inertial,action)\n",
    "    testing_data_dept_f += depth_model.fetch_training_data_by_action(testing_data_depth,action)\n",
    "\n",
    "fuse_predicted = fuse_models(actions,[testing_data_skel_f,testing_data_iner_f,testing_data_dept_f], [skeleton_model,inertial_model, depth_model])\n",
    "_,fuse_actual_iner = zip(*[(testx,testy) for testx,testy in testing_data_iner_f if testy in actions])\n",
    "_,fuse_actual_skel = zip(*[(testx,testy) for testx,testy in testing_data_skel_f if testy in actions])\n",
    "_,fuse_actual_dept = zip(*[(testx,testy) for testx,testy in testing_data_dept_f if testy in actions])\n",
    "\n",
    "print(fuse_predicted,fuse_actual_iner,fuse_actual_skel,fuse_actual_dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "skeleton_model.print_classification_report(fuse_actual_iner,fuse_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0u1mnks9Mne1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Future Research Considerations\t\n",
    "<small>\n",
    "In order to realize the full potential of HAR (Human Activity Recognition) systems, certain topics like the one’s mentioned below need further investigation.\n",
    "\n",
    "**Activity Recognition Data Set**: The quantitative comparison of HAR approaches has been hindered by the fact that each system works with a different dataset. In that direction, various datasets publicly open to the research community can be included which can be used as benchmarks to evaluate new approaches. \n",
    " \n",
    "**Concurrent and overlapping activities**: The assumption that an individual only performs one activity at a time is true for basic ambulation activities. In general, human activities are rather overlapping and concurrent. Since only few works have been reported in this area, we foresee great research opportunities in this field. \n",
    " \n",
    "**Crowd HAR**: The recognition of human activities has been somehow individualized, i.e., the majority of the systems predict activities in a single user. If we could gather activity patterns from a significant sample of people in certain area (e.g., a city, a state, or a country), that information could be used to estimate levels of sedentarism , exercise habits, and even health conditions in a target population. \n",
    "<small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tAZiQQsUEqQ-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thank You :)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "combined_models.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "livereveal": {
   "start_slideshow_at": "selected",
   "theme": "sky",
   "transition": "zoom"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
