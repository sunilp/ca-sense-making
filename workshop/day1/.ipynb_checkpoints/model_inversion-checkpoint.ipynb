{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dnn_utils import *\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 12288     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost =  compute_cost(A2, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, \n",
    "                                             parameters['W' + str(l)], \n",
    "                                             parameters['b' + str(l)], \n",
    "                                             activation='relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, \n",
    "                                          parameters['W' + str(L)], \n",
    "                                          parameters['b' + str(L)], \n",
    "                                          activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    #print(AL.shape, (2, X.shape[1]) )\n",
    "    assert(AL.shape == (2, X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layers_dims = [12288, 20, 7, 5, 1] #  5-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generate Training Data\n",
    "\n",
    "xe = l1 cos(theta1)  + l2 cos(theta1 + theta2) + l3 cos(theta1 + theta2 + theta3)\n",
    "\n",
    "ye = l1 sin(theta1) + l2 sin(theta1 + theta2) + l3 sin(theta1 + theta2 + theta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_XY_pair(l1,l2,l3, theta1, theta2, theta3):\n",
    "    \"\"\"\n",
    "    Generates a pair of X and Y on provided angles of the robotic arm\n",
    "    \n",
    "    Arguments:\n",
    "    l1 -- length of first arm\n",
    "    l2 -- length of second arm\n",
    "    l3 -- length of third arm\n",
    "    theta1 -- angle from base of first arm\n",
    "    theta2 -- angle from base of second arm\n",
    "    theta3 -- angle from base of third arm\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    xe = l1 * np.cos(theta1) + l2 * np.cos(theta1 + theta2) + l3 * np.cos(theta1 + theta2 + theta3)\n",
    "    ye = l1 * np.sin(theta1) + l2 * np.sin(theta1 + theta2) + l3 * np.sin(theta1 + theta2 + theta3)\n",
    "    return (xe,ye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angles are 3.025046976993713 3.4291312495246884 3.10999300732959\n",
      "X and Y are -1.99328507864 0.0397768662649\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test the generate_XY_pair\n",
    "l1 = 1\n",
    "l2 = 2\n",
    "l3 = 3\n",
    "\n",
    "std_dev = 0.1\n",
    "\n",
    "theta1 = np.pi + np.random.randn() * std_dev\n",
    "theta2 = np.pi + np.random.randn() * std_dev\n",
    "theta3 = np.pi + np.random.randn() * std_dev\n",
    "\n",
    "x,y = generate_XY_pair(l1,l2,l3,theta1,theta2,theta3)\n",
    "print(\"angles are\", theta1, theta2, theta3)\n",
    "print(\"X and Y are\", x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets generate 10000 records\n",
    "\n",
    "increment = 0.1\n",
    "no_of_records = 10000\n",
    "\n",
    "data=[]\n",
    "\n",
    "theta1=theta2=theta3=-np.pi\n",
    "\n",
    "for i in range(0, 70):\n",
    "    #print(i)\n",
    "    if theta1 < np.pi:\n",
    "        theta1 +=increment\n",
    "        theta2=theta3=-np.pi\n",
    "        for j in range(0, 70):\n",
    "            if theta2 < np.pi:\n",
    "                theta2 +=increment\n",
    "                theta3=-np.pi\n",
    "                for k in range(0,70):\n",
    "                    if theta3 < np.pi:\n",
    "                        theta3 +=increment\n",
    "                        data.append([theta1,theta2,theta3])\n",
    "\n",
    "#theta1 = np.pi + np.random.randn(no_of_records) * std_dev\n",
    "#theta2 = np.pi + np.random.randn(no_of_records) * std_dev\n",
    "#theta3 = np.pi + np.random.randn(no_of_records) * std_dev\n",
    "\n",
    "\n",
    "\n",
    "l1 = np.zeros(len(data)) + 1\n",
    "l2 = np.zeros(len(data)) + 2\n",
    "l3 = np.zeros(len(data)) + 3\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.04159265, -3.04159265, -3.04159265],\n",
       "       [-3.04159265, -3.04159265, -2.94159265],\n",
       "       [-3.04159265, -3.04159265, -2.84159265],\n",
       "       ..., \n",
       "       [ 3.15840735,  3.15840735,  2.95840735],\n",
       "       [ 3.15840735,  3.15840735,  3.05840735],\n",
       "       [ 3.15840735,  3.15840735,  3.15840735]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250047\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "\n",
    "x,y = generate_XY_pair(l1,l2,l3,data[:,0],data[:,1],data[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.87074978, -1.14077137, -1.39642218, -1.63514782, -1.85456303,\n",
       "       -2.05247548, -2.22690771, -2.37611684, -2.49861201])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:10]\n",
    "y[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " values are 5.99793154915 5.9977765609 -5.99754847696 -5.99624414369\n",
      "[-0.29982052 -0.27807311 -0.2519398  -0.22168168 -0.18760111 -0.15003859\n",
      " -0.10936944 -0.06600001 -0.02036364]\n"
     ]
    }
   ],
   "source": [
    "#scale the x and y as min-max scaling\n",
    "\n",
    "x_max = np.max(x)\n",
    "x_min = np.min(x)\n",
    "\n",
    "y_max = np.max(y)\n",
    "y_min = np.min(y)\n",
    "print(\" values are\", x_max, y_max, x_min, y_min)\n",
    "\n",
    "#scale it\n",
    "x_norm = 2 * ((x - x_min)/ (x_max-x_min)) - 1\n",
    "y_norm = 2 * ((y - y_min) / (y_max - y_min)) - 1\n",
    "\n",
    "print(x_norm[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_x = np.column_stack((theta1,theta2,theta3)).T\n",
    "train_x = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 250047)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = np.column_stack((x_norm,y_norm)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 250047)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers_dims = [3, 10, 20, 10, 2] #  5-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.386295\n",
      "Cost after iteration 100: 1.072590\n",
      "Cost after iteration 200: 0.854588\n",
      "Cost after iteration 300: 0.699537\n",
      "Cost after iteration 400: 0.586128\n",
      "Cost after iteration 500: 0.500823\n",
      "Cost after iteration 600: 0.434976\n",
      "Cost after iteration 700: 0.382955\n",
      "Cost after iteration 800: 0.341008\n",
      "Cost after iteration 900: 0.306568\n",
      "Cost after iteration 1000: 0.277842\n",
      "Cost after iteration 1100: 0.253542\n",
      "Cost after iteration 1200: 0.232730\n",
      "Cost after iteration 1300: 0.214707\n",
      "Cost after iteration 1400: 0.198943\n",
      "Cost after iteration 1500: 0.185032\n",
      "Cost after iteration 1600: 0.172658\n",
      "Cost after iteration 1700: 0.161570\n",
      "Cost after iteration 1800: 0.151569\n",
      "Cost after iteration 1900: 0.142496\n",
      "Cost after iteration 2000: 0.134221\n",
      "Cost after iteration 2100: 0.126636\n",
      "Cost after iteration 2200: 0.119655\n",
      "Cost after iteration 2300: 0.113204\n",
      "Cost after iteration 2400: 0.107222\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOYPaEoaxygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118c1a048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.0177994 ,  0.00426454,  0.000934  ],\n",
       "        [-0.01896168, -0.00296312, -0.00359468],\n",
       "        [-0.00085928, -0.0063077 , -0.00043228],\n",
       "        [-0.00503953, -0.01329157,  0.00890885],\n",
       "        [ 0.00875989,  0.01705744,  0.00049753],\n",
       "        [-0.00412212, -0.0054819 , -0.01547433],\n",
       "        [ 0.00985497, -0.01099701, -0.01186139],\n",
       "        [-0.00201888,  0.01492457,  0.00237502],\n",
       "        [-0.0102983 , -0.00717661,  0.00623851],\n",
       "        [-0.00162964, -0.00764539, -0.00227443]]),\n",
       " 'W2': array([[  7.45017086e-03,   1.97225393e-02,  -1.24464765e-02,\n",
       "          -6.29140961e-03,  -8.03782535e-03,  -2.41942840e-02,\n",
       "          -9.23798278e-03,  -1.02437895e-02,   1.12073176e-02,\n",
       "          -1.32487834e-03],\n",
       "        [ -1.62328981e-02,   6.48107218e-03,  -3.56268503e-03,\n",
       "          -1.74314196e-02,  -5.96927658e-03,  -5.87463447e-03,\n",
       "          -8.73862591e-03,   2.98777424e-04,  -2.24811641e-02,\n",
       "          -2.67732484e-03],\n",
       "        [  1.01123652e-02,   8.51874484e-03,   1.10803096e-02,\n",
       "           1.11829434e-02,   1.48464797e-02,  -1.11824285e-02,\n",
       "           8.46329357e-03,  -1.86221832e-02,  -6.04148990e-03,\n",
       "          -1.91466387e-02],\n",
       "        [  1.05331383e-02,   1.34428653e-02,  -1.95373201e-03,\n",
       "           1.78033554e-02,  -6.68707169e-03,   1.56187720e-03,\n",
       "           1.55345570e-03,  -1.06054784e-02,   4.44596276e-03,\n",
       "           1.94186454e-02],\n",
       "        [ -1.02484244e-02,   8.99703248e-03,  -1.54446576e-03,\n",
       "           1.76972002e-02,   4.83905272e-03,   6.76404319e-03,\n",
       "           6.43137311e-03,   2.49128136e-03,  -1.39562379e-02,\n",
       "           1.39176189e-02],\n",
       "        [ -1.37070213e-02,   2.37181035e-03,   6.13962360e-03,\n",
       "          -8.37925457e-03,   1.44883560e-03,   1.16599119e-02,\n",
       "          -2.48653034e-04,  -8.89036349e-03,  -2.91578788e-02,\n",
       "          -9.72114464e-03],\n",
       "        [ -5.91637464e-03,  -5.16875965e-03,  -9.61028336e-03,\n",
       "           3.75996309e-03,  -5.74713057e-03,  -1.11519084e-03,\n",
       "           6.75760418e-03,  -8.55437201e-03,  -3.00813318e-03,\n",
       "           2.15676785e-02],\n",
       "        [  8.69402341e-03,  -1.29365876e-02,  -7.98689030e-04,\n",
       "           5.63952747e-03,   1.22844083e-02,   1.48786595e-03,\n",
       "          -5.31684734e-03,  -7.32714282e-03,   6.44769194e-03,\n",
       "           3.12937257e-03],\n",
       "        [ -5.16224516e-03,  -1.89817331e-03,  -4.16661789e-03,\n",
       "           7.24076602e-03,  -6.89965434e-03,   4.84887036e-03,\n",
       "           8.50411270e-03,   4.86181074e-03,  -8.34714290e-03,\n",
       "           1.34429171e-02],\n",
       "        [ -6.78235000e-03,   4.24753160e-03,  -7.53341372e-03,\n",
       "          -1.74411126e-02,   2.25480732e-03,   2.85764902e-03,\n",
       "          -7.75039436e-04,   2.75249530e-03,  -6.48556812e-03,\n",
       "          -7.37500819e-03],\n",
       "        [ -1.60893650e-03,   1.92568409e-02,   8.17718534e-03,\n",
       "          -5.11429010e-03,   5.68169418e-03,  -4.70170602e-03,\n",
       "          -4.54386111e-03,   8.65788224e-03,  -5.14985873e-03,\n",
       "          -1.67153787e-02],\n",
       "        [ -8.97815408e-03,   1.07699149e-03,   1.31668656e-03,\n",
       "           1.25215460e-02,  -7.05549447e-03,   7.41368024e-03,\n",
       "           4.30092530e-03,  -1.42275596e-03,   8.48184964e-03,\n",
       "           4.97257254e-03],\n",
       "        [ -8.62307488e-03,   1.06963019e-02,  -1.22117617e-02,\n",
       "           5.86094470e-04,   2.54024137e-05,   4.23799426e-03,\n",
       "          -7.25571173e-03,  -3.50685711e-04,  -1.41875741e-03,\n",
       "           9.96716943e-03],\n",
       "        [ -7.93303781e-03,   7.56889182e-04,  -2.61088281e-03,\n",
       "          -1.29761798e-02,   2.67883203e-02,  -6.97690319e-04,\n",
       "          -1.48636743e-02,   1.41050179e-02,  -1.06960175e-02,\n",
       "           3.71168315e-03],\n",
       "        [  8.61213327e-03,  -6.48452788e-03,  -4.30907436e-03,\n",
       "          -5.40397383e-03,  -1.30792866e-03,  -1.62246171e-02,\n",
       "          -1.23575697e-02,  -1.41351508e-03,   1.03889439e-02,\n",
       "           6.31730072e-03],\n",
       "        [  1.72894120e-02,   6.91994577e-03,  -5.11343075e-03,\n",
       "          -1.24392253e-03,  -2.03053291e-02,  -9.60861521e-03,\n",
       "          -1.02043990e-02,   2.70057122e-03,   6.46075263e-03,\n",
       "          -5.60554754e-03],\n",
       "        [ -5.86880746e-03,  -1.54674347e-02,  -1.27620111e-03,\n",
       "           2.48201042e-03,   4.47559250e-03,  -7.82379563e-03,\n",
       "           1.98903038e-02,   1.19562324e-02,  -9.52052639e-04,\n",
       "          -5.26936206e-03],\n",
       "        [ -3.21535745e-03,   1.52152553e-03,  -1.84674187e-04,\n",
       "           4.84065903e-03,   7.69290622e-03,   1.36670916e-02,\n",
       "           1.14736225e-02,  -1.09760999e-03,   3.88894929e-03,\n",
       "          -3.86912795e-03],\n",
       "        [ -5.78875393e-03,   1.92768695e-02,  -4.56623276e-03,\n",
       "           1.99991003e-02,  -3.39066819e-03,   2.61593943e-03,\n",
       "           1.09322808e-02,   3.01567332e-04,   4.03861672e-03,\n",
       "          -2.36849218e-03],\n",
       "        [ -4.74998598e-03,  -1.62625477e-03,  -6.49194241e-03,\n",
       "           1.63289479e-02,  -1.67117641e-03,   1.72410160e-02,\n",
       "          -2.68472841e-02,   1.91278197e-04,   5.63746752e-03,\n",
       "          -2.93099494e-03]]),\n",
       " 'W3': array([[ 0.0109465 ,  0.00639692, -0.00274605,  0.00434907,  0.02811828,\n",
       "          0.00251995,  0.00299499, -0.00439994,  0.0013348 , -0.01289262,\n",
       "         -0.00198352,  0.02457481,  0.01067198,  0.00641416,  0.01103921,\n",
       "          0.01881753,  0.00593586,  0.0207083 ,  0.01069671,  0.00166495],\n",
       "        [ 0.01719475, -0.02359215, -0.0057135 ,  0.00265775, -0.00912095,\n",
       "         -0.00156058, -0.00638795, -0.00654416,  0.02711918,  0.00627474,\n",
       "         -0.00053954,  0.01315139, -0.00237334,  0.00885339,  0.00350816,\n",
       "          0.01626573, -0.01419891,  0.00765714,  0.0012223 , -0.01157377],\n",
       "        [ 0.01065403, -0.00872374,  0.01619287,  0.0051359 ,  0.0069585 ,\n",
       "          0.00080459,  0.00904523, -0.01865655,  0.00074738, -0.00628247,\n",
       "          0.00282815, -0.0004704 ,  0.00616827, -0.0083763 ,  0.01839152,\n",
       "          0.0231584 , -0.00208298, -0.00014717,  0.00287883,  0.01264124],\n",
       "        [ 0.01896901, -0.01205803, -0.00615109, -0.01062156, -0.01112782,\n",
       "         -0.01639296,  0.00362803, -0.01159037,  0.01503262,  0.00908318,\n",
       "         -0.01029711, -0.01030209, -0.00612385,  0.01399962, -0.00849607,\n",
       "         -0.0149355 , -0.00049425,  0.00373389, -0.00657258,  0.01619431],\n",
       "        [ 0.00243221,  0.00453336, -0.00850368,  0.00023569, -0.00141475,\n",
       "         -0.02271918,  0.00288258, -0.01793993, -0.00025774, -0.01473489,\n",
       "          0.02093653,  0.00407214,  0.00865802,  0.00936255, -0.01324112,\n",
       "         -0.02281496, -0.00326099,  0.00914895,  0.00181643,  0.00803294],\n",
       "        [ 0.00936632, -0.01492397,  0.00287683,  0.0196648 , -0.00570533,\n",
       "         -0.02029072, -0.00231905, -0.0046467 ,  0.00418381, -0.00892406,\n",
       "          0.0009073 , -0.02217409,  0.00853963,  0.01586872,  0.01297877,\n",
       "         -0.01515216,  0.00319198, -0.02983969,  0.00283195, -0.0006436 ],\n",
       "        [-0.00991979,  0.00344132,  0.00146335,  0.01027806,  0.00147704,\n",
       "          0.00235455, -0.0194431 , -0.01158497, -0.0047198 ,  0.0029738 ,\n",
       "          0.00096036,  0.01612007, -0.00863923, -0.00215271,  0.00255578,\n",
       "         -0.00231068,  0.00501115, -0.00545726,  0.01545126, -0.00295267],\n",
       "        [ 0.01115941, -0.00030315,  0.01491757, -0.01398246,  0.00516517,\n",
       "         -0.00432585,  0.00231373,  0.01192792, -0.01139212, -0.01322033,\n",
       "         -0.00998298,  0.00254013, -0.01886863,  0.00096739, -0.01286251,\n",
       "         -0.01143712, -0.00369038,  0.00380497, -0.00626715, -0.00492205],\n",
       "        [-0.00041844, -0.00272736, -0.02676522, -0.00430104,  0.0008496 ,\n",
       "          0.01097779,  0.0204633 ,  0.00666988,  0.00079088, -0.00964764,\n",
       "          0.00089053,  0.00778894,  0.01264645, -0.00880511,  0.00236406,\n",
       "          0.00815604,  0.01860809,  0.00255585, -0.00541508, -0.006896  ],\n",
       "        [-0.00357444, -0.0065192 ,  0.00826531,  0.01069236,  0.00724828,\n",
       "          0.01192186, -0.0045377 ,  0.00380333, -0.00384672,  0.00043658,\n",
       "          0.01224941, -0.00029809, -0.01864815, -0.0025282 , -0.0071285 ,\n",
       "         -0.01508918, -0.00790368,  0.00960597,  0.01680823, -0.00489025]]),\n",
       " 'W4': array([[ 0.01002512,  0.01178229, -0.01161396, -0.00039364, -0.17622476,\n",
       "          0.00172387, -0.25955221, -0.00352038,  0.01057818,  0.01262193],\n",
       "        [ 0.01831335, -0.00337523,  0.0186898 ,  0.00665904, -0.18912027,\n",
       "          0.0076161 , -0.24678461,  0.00518434, -0.00102527,  0.01208227]]),\n",
       " 'b1': array([[ -2.21771776e-05],\n",
       "        [  1.90935817e-04],\n",
       "        [  2.75749205e-05],\n",
       "        [  1.31922903e-04],\n",
       "        [ -1.76874045e-05],\n",
       "        [  3.42986346e-05],\n",
       "        [  1.61916439e-05],\n",
       "        [  3.96227465e-05],\n",
       "        [  3.54412123e-05],\n",
       "        [ -1.97735226e-05]]),\n",
       " 'b2': array([[ -9.30518436e-04],\n",
       "        [  1.74684818e-04],\n",
       "        [ -5.84412305e-04],\n",
       "        [  3.33433407e-03],\n",
       "        [  2.28244173e-05],\n",
       "        [ -4.84002727e-04],\n",
       "        [ -6.95334880e-04],\n",
       "        [ -1.07428262e-03],\n",
       "        [ -3.84022489e-04],\n",
       "        [ -3.76977735e-04],\n",
       "        [  5.19633474e-03],\n",
       "        [  6.01756852e-03],\n",
       "        [ -4.47073461e-04],\n",
       "        [  9.46385822e-04],\n",
       "        [ -3.50369733e-04],\n",
       "        [ -6.07913143e-04],\n",
       "        [  1.50735201e-04],\n",
       "        [  3.24103760e-04],\n",
       "        [  5.41776546e-03],\n",
       "        [  8.14180517e-04]]),\n",
       " 'b3': array([[ -3.70943648e-04],\n",
       "        [ -1.12231573e-04],\n",
       "        [ -6.26220466e-05],\n",
       "        [ -1.95660107e-05],\n",
       "        [  2.58052385e-01],\n",
       "        [  1.02167246e-06],\n",
       "        [  3.57662113e-01],\n",
       "        [  1.33715054e-04],\n",
       "        [ -4.21432434e-05],\n",
       "        [ -2.48402066e-04]]),\n",
       " 'b4': array([[-2.79895473],\n",
       "        [-2.79616536]])}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(test_x, parameters):\n",
    "    return L_model_forward(test_x, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "test_x = np.array([[0.,0.,0.]]).T\n",
    "print(test_x.shape)\n",
    "test_y = [6]\n",
    "predicted_y,cache = predict(test_x, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05033797],\n",
       "       [ 0.05053092]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-5.39372039]), array([-5.3901753]))"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_yx = predicted_y[0] * (x_max-x_min) + x_min\n",
    "scaled_yy = predicted_y[1] * (y_max-y_min) + y_min\n",
    "(scaled_yx,scaled_yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param2 = np.copy(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'W1': array([[ 0.0177994 ,  0.00426454,  0.000934  ],\n",
       "       [-0.01896168, -0.00296312, -0.00359468],\n",
       "       [-0.00085928, -0.0063077 , -0.00043228],\n",
       "       [-0.00503953, -0.01329157,  0.00890885],\n",
       "       [ 0.00875989,  0.01705744,  0.00049753],\n",
       "       [-0.00412212, -0.0054819 , -0.01547433],\n",
       "       [ 0.00985497, -0.01099701, -0.01186139],\n",
       "       [-0.00201888,  0.01492457,  0.00237502],\n",
       "       [-0.0102983 , -0.00717661,  0.00623851],\n",
       "       [-0.00162964, -0.00764539, -0.00227443]]), 'b1': array([[ -2.21771776e-05],\n",
       "       [  1.90935817e-04],\n",
       "       [  2.75749205e-05],\n",
       "       [  1.31922903e-04],\n",
       "       [ -1.76874045e-05],\n",
       "       [  3.42986346e-05],\n",
       "       [  1.61916439e-05],\n",
       "       [  3.96227465e-05],\n",
       "       [  3.54412123e-05],\n",
       "       [ -1.97735226e-05]]), 'W2': array([[  7.45017086e-03,   1.97225393e-02,  -1.24464765e-02,\n",
       "         -6.29140961e-03,  -8.03782535e-03,  -2.41942840e-02,\n",
       "         -9.23798278e-03,  -1.02437895e-02,   1.12073176e-02,\n",
       "         -1.32487834e-03],\n",
       "       [ -1.62328981e-02,   6.48107218e-03,  -3.56268503e-03,\n",
       "         -1.74314196e-02,  -5.96927658e-03,  -5.87463447e-03,\n",
       "         -8.73862591e-03,   2.98777424e-04,  -2.24811641e-02,\n",
       "         -2.67732484e-03],\n",
       "       [  1.01123652e-02,   8.51874484e-03,   1.10803096e-02,\n",
       "          1.11829434e-02,   1.48464797e-02,  -1.11824285e-02,\n",
       "          8.46329357e-03,  -1.86221832e-02,  -6.04148990e-03,\n",
       "         -1.91466387e-02],\n",
       "       [  1.05331383e-02,   1.34428653e-02,  -1.95373201e-03,\n",
       "          1.78033554e-02,  -6.68707169e-03,   1.56187720e-03,\n",
       "          1.55345570e-03,  -1.06054784e-02,   4.44596276e-03,\n",
       "          1.94186454e-02],\n",
       "       [ -1.02484244e-02,   8.99703248e-03,  -1.54446576e-03,\n",
       "          1.76972002e-02,   4.83905272e-03,   6.76404319e-03,\n",
       "          6.43137311e-03,   2.49128136e-03,  -1.39562379e-02,\n",
       "          1.39176189e-02],\n",
       "       [ -1.37070213e-02,   2.37181035e-03,   6.13962360e-03,\n",
       "         -8.37925457e-03,   1.44883560e-03,   1.16599119e-02,\n",
       "         -2.48653034e-04,  -8.89036349e-03,  -2.91578788e-02,\n",
       "         -9.72114464e-03],\n",
       "       [ -5.91637464e-03,  -5.16875965e-03,  -9.61028336e-03,\n",
       "          3.75996309e-03,  -5.74713057e-03,  -1.11519084e-03,\n",
       "          6.75760418e-03,  -8.55437201e-03,  -3.00813318e-03,\n",
       "          2.15676785e-02],\n",
       "       [  8.69402341e-03,  -1.29365876e-02,  -7.98689030e-04,\n",
       "          5.63952747e-03,   1.22844083e-02,   1.48786595e-03,\n",
       "         -5.31684734e-03,  -7.32714282e-03,   6.44769194e-03,\n",
       "          3.12937257e-03],\n",
       "       [ -5.16224516e-03,  -1.89817331e-03,  -4.16661789e-03,\n",
       "          7.24076602e-03,  -6.89965434e-03,   4.84887036e-03,\n",
       "          8.50411270e-03,   4.86181074e-03,  -8.34714290e-03,\n",
       "          1.34429171e-02],\n",
       "       [ -6.78235000e-03,   4.24753160e-03,  -7.53341372e-03,\n",
       "         -1.74411126e-02,   2.25480732e-03,   2.85764902e-03,\n",
       "         -7.75039436e-04,   2.75249530e-03,  -6.48556812e-03,\n",
       "         -7.37500819e-03],\n",
       "       [ -1.60893650e-03,   1.92568409e-02,   8.17718534e-03,\n",
       "         -5.11429010e-03,   5.68169418e-03,  -4.70170602e-03,\n",
       "         -4.54386111e-03,   8.65788224e-03,  -5.14985873e-03,\n",
       "         -1.67153787e-02],\n",
       "       [ -8.97815408e-03,   1.07699149e-03,   1.31668656e-03,\n",
       "          1.25215460e-02,  -7.05549447e-03,   7.41368024e-03,\n",
       "          4.30092530e-03,  -1.42275596e-03,   8.48184964e-03,\n",
       "          4.97257254e-03],\n",
       "       [ -8.62307488e-03,   1.06963019e-02,  -1.22117617e-02,\n",
       "          5.86094470e-04,   2.54024137e-05,   4.23799426e-03,\n",
       "         -7.25571173e-03,  -3.50685711e-04,  -1.41875741e-03,\n",
       "          9.96716943e-03],\n",
       "       [ -7.93303781e-03,   7.56889182e-04,  -2.61088281e-03,\n",
       "         -1.29761798e-02,   2.67883203e-02,  -6.97690319e-04,\n",
       "         -1.48636743e-02,   1.41050179e-02,  -1.06960175e-02,\n",
       "          3.71168315e-03],\n",
       "       [  8.61213327e-03,  -6.48452788e-03,  -4.30907436e-03,\n",
       "         -5.40397383e-03,  -1.30792866e-03,  -1.62246171e-02,\n",
       "         -1.23575697e-02,  -1.41351508e-03,   1.03889439e-02,\n",
       "          6.31730072e-03],\n",
       "       [  1.72894120e-02,   6.91994577e-03,  -5.11343075e-03,\n",
       "         -1.24392253e-03,  -2.03053291e-02,  -9.60861521e-03,\n",
       "         -1.02043990e-02,   2.70057122e-03,   6.46075263e-03,\n",
       "         -5.60554754e-03],\n",
       "       [ -5.86880746e-03,  -1.54674347e-02,  -1.27620111e-03,\n",
       "          2.48201042e-03,   4.47559250e-03,  -7.82379563e-03,\n",
       "          1.98903038e-02,   1.19562324e-02,  -9.52052639e-04,\n",
       "         -5.26936206e-03],\n",
       "       [ -3.21535745e-03,   1.52152553e-03,  -1.84674187e-04,\n",
       "          4.84065903e-03,   7.69290622e-03,   1.36670916e-02,\n",
       "          1.14736225e-02,  -1.09760999e-03,   3.88894929e-03,\n",
       "         -3.86912795e-03],\n",
       "       [ -5.78875393e-03,   1.92768695e-02,  -4.56623276e-03,\n",
       "          1.99991003e-02,  -3.39066819e-03,   2.61593943e-03,\n",
       "          1.09322808e-02,   3.01567332e-04,   4.03861672e-03,\n",
       "         -2.36849218e-03],\n",
       "       [ -4.74998598e-03,  -1.62625477e-03,  -6.49194241e-03,\n",
       "          1.63289479e-02,  -1.67117641e-03,   1.72410160e-02,\n",
       "         -2.68472841e-02,   1.91278197e-04,   5.63746752e-03,\n",
       "         -2.93099494e-03]]), 'b2': array([[ -9.30518436e-04],\n",
       "       [  1.74684818e-04],\n",
       "       [ -5.84412305e-04],\n",
       "       [  3.33433407e-03],\n",
       "       [  2.28244173e-05],\n",
       "       [ -4.84002727e-04],\n",
       "       [ -6.95334880e-04],\n",
       "       [ -1.07428262e-03],\n",
       "       [ -3.84022489e-04],\n",
       "       [ -3.76977735e-04],\n",
       "       [  5.19633474e-03],\n",
       "       [  6.01756852e-03],\n",
       "       [ -4.47073461e-04],\n",
       "       [  9.46385822e-04],\n",
       "       [ -3.50369733e-04],\n",
       "       [ -6.07913143e-04],\n",
       "       [  1.50735201e-04],\n",
       "       [  3.24103760e-04],\n",
       "       [  5.41776546e-03],\n",
       "       [  8.14180517e-04]]), 'W3': array([[ 0.0109465 ,  0.00639692, -0.00274605,  0.00434907,  0.02811828,\n",
       "         0.00251995,  0.00299499, -0.00439994,  0.0013348 , -0.01289262,\n",
       "        -0.00198352,  0.02457481,  0.01067198,  0.00641416,  0.01103921,\n",
       "         0.01881753,  0.00593586,  0.0207083 ,  0.01069671,  0.00166495],\n",
       "       [ 0.01719475, -0.02359215, -0.0057135 ,  0.00265775, -0.00912095,\n",
       "        -0.00156058, -0.00638795, -0.00654416,  0.02711918,  0.00627474,\n",
       "        -0.00053954,  0.01315139, -0.00237334,  0.00885339,  0.00350816,\n",
       "         0.01626573, -0.01419891,  0.00765714,  0.0012223 , -0.01157377],\n",
       "       [ 0.01065403, -0.00872374,  0.01619287,  0.0051359 ,  0.0069585 ,\n",
       "         0.00080459,  0.00904523, -0.01865655,  0.00074738, -0.00628247,\n",
       "         0.00282815, -0.0004704 ,  0.00616827, -0.0083763 ,  0.01839152,\n",
       "         0.0231584 , -0.00208298, -0.00014717,  0.00287883,  0.01264124],\n",
       "       [ 0.01896901, -0.01205803, -0.00615109, -0.01062156, -0.01112782,\n",
       "        -0.01639296,  0.00362803, -0.01159037,  0.01503262,  0.00908318,\n",
       "        -0.01029711, -0.01030209, -0.00612385,  0.01399962, -0.00849607,\n",
       "        -0.0149355 , -0.00049425,  0.00373389, -0.00657258,  0.01619431],\n",
       "       [ 0.00243221,  0.00453336, -0.00850368,  0.00023569, -0.00141475,\n",
       "        -0.02271918,  0.00288258, -0.01793993, -0.00025774, -0.01473489,\n",
       "         0.02093653,  0.00407214,  0.00865802,  0.00936255, -0.01324112,\n",
       "        -0.02281496, -0.00326099,  0.00914895,  0.00181643,  0.00803294],\n",
       "       [ 0.00936632, -0.01492397,  0.00287683,  0.0196648 , -0.00570533,\n",
       "        -0.02029072, -0.00231905, -0.0046467 ,  0.00418381, -0.00892406,\n",
       "         0.0009073 , -0.02217409,  0.00853963,  0.01586872,  0.01297877,\n",
       "        -0.01515216,  0.00319198, -0.02983969,  0.00283195, -0.0006436 ],\n",
       "       [-0.00991979,  0.00344132,  0.00146335,  0.01027806,  0.00147704,\n",
       "         0.00235455, -0.0194431 , -0.01158497, -0.0047198 ,  0.0029738 ,\n",
       "         0.00096036,  0.01612007, -0.00863923, -0.00215271,  0.00255578,\n",
       "        -0.00231068,  0.00501115, -0.00545726,  0.01545126, -0.00295267],\n",
       "       [ 0.01115941, -0.00030315,  0.01491757, -0.01398246,  0.00516517,\n",
       "        -0.00432585,  0.00231373,  0.01192792, -0.01139212, -0.01322033,\n",
       "        -0.00998298,  0.00254013, -0.01886863,  0.00096739, -0.01286251,\n",
       "        -0.01143712, -0.00369038,  0.00380497, -0.00626715, -0.00492205],\n",
       "       [-0.00041844, -0.00272736, -0.02676522, -0.00430104,  0.0008496 ,\n",
       "         0.01097779,  0.0204633 ,  0.00666988,  0.00079088, -0.00964764,\n",
       "         0.00089053,  0.00778894,  0.01264645, -0.00880511,  0.00236406,\n",
       "         0.00815604,  0.01860809,  0.00255585, -0.00541508, -0.006896  ],\n",
       "       [-0.00357444, -0.0065192 ,  0.00826531,  0.01069236,  0.00724828,\n",
       "         0.01192186, -0.0045377 ,  0.00380333, -0.00384672,  0.00043658,\n",
       "         0.01224941, -0.00029809, -0.01864815, -0.0025282 , -0.0071285 ,\n",
       "        -0.01508918, -0.00790368,  0.00960597,  0.01680823, -0.00489025]]), 'b3': array([[ -3.70943648e-04],\n",
       "       [ -1.12231573e-04],\n",
       "       [ -6.26220466e-05],\n",
       "       [ -1.95660107e-05],\n",
       "       [  2.58052385e-01],\n",
       "       [  1.02167246e-06],\n",
       "       [  3.57662113e-01],\n",
       "       [  1.33715054e-04],\n",
       "       [ -4.21432434e-05],\n",
       "       [ -2.48402066e-04]]), 'W4': array([[ 0.01002512,  0.01178229, -0.01161396, -0.00039364, -0.17622476,\n",
       "         0.00172387, -0.25955221, -0.00352038,  0.01057818,  0.01262193],\n",
       "       [ 0.01831335, -0.00337523,  0.0186898 ,  0.00665904, -0.18912027,\n",
       "         0.0076161 , -0.24678461,  0.00518434, -0.00102527,  0.01208227]]), 'b4': array([[-2.79895473],\n",
       "       [-2.79616536]])}, dtype=object)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def L_layer_model_reversed(X, Y, params,layers_dims, learning_rate = 0.01, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    params = initialize_parameters_deep_rev(layers_dims,params)\n",
    "    ### END CODE HERE ###\n",
    "    #parameters = params\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, 4):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        #print(\"current params\", params)\n",
    "        AL, caches = L_model_forward_inverse(X, params)\n",
    "        ### END CODE HERE ###\n",
    "        print(\"AL\", AL,Y)\n",
    "        \n",
    "        # Compute cost.\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        print(\"cost\", cost)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    "        for key in list(grads):\n",
    "            if key.startswith(\"dW\") or key.startswith(\"db\"):\n",
    "                #print(\"grads are\", key)\n",
    "                grads.pop(key,None)\n",
    "    \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        params = update_parameters_reversed(params, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        #print(\"grads\", grads)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "  \n",
    "    \n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_reversed(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "                  parameters[\"W\" + str(l)] = ...\n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "    #print(\"L is \", L, parameters)\n",
    "    #print(\"grads is \", L, grads)\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (˜ 3 lines of code)\n",
    "    for l in range(L-2):\n",
    "        #print(l,parameters[\"A\" + str(l )].shape,grads[\"dA\" + str(l + 1)])\n",
    "        parameters[\"A\" + str(l )] = parameters[\"A\" + str(l )] - learning_rate * grads[\"dA\" + str(l + 1)]\n",
    "       # parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    ### END CODE HERE ###\n",
    "    print(\"L is \", L, parameters)\n",
    "    parameters[\"A\" + str(l )] = parameters[\"A\" + str(l )] - learning_rate * grads[\"dA\" + str(l + 1)]\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep_rev(layer_dims,parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)\n",
    "    #parameters = {}\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "\n",
    "    for l in range(0, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        #parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['A' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        #assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        #assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward_inverse(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, 4):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        #print(\"l is\", l)\n",
    "        A, cache = linear_activation_forward(A_prev, \n",
    "                                             parameters['W' + str(l)], \n",
    "                                             parameters['b' + str(l)], \n",
    "                                             activation='relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    L=4\n",
    "    AL, cache = linear_activation_forward(A, \n",
    "                                          parameters['W' + str(L)], \n",
    "                                          parameters['b' + str(L)], \n",
    "                                          activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    #print(AL.shape, (2, X.shape[1]) )\n",
    "    assert(AL.shape == (2, X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL [[ 0.05033797]\n",
      " [ 0.05053092]] [[ 0.]\n",
      " [ 6.]]\n",
      "cost 17.703407201\n",
      "AL [[ 0.05033797]\n",
      " [ 0.05053092]] [[ 0.]\n",
      " [ 6.]]\n",
      "cost 17.703407201\n",
      "AL [[ 0.05033797]\n",
      " [ 0.05053092]] [[ 0.]\n",
      " [ 6.]]\n",
      "cost 17.703407201\n",
      "AL [[ 0.05033797]\n",
      " [ 0.05053092]] [[ 0.]\n",
      " [ 6.]]\n",
      "cost 17.703407201\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([[0.,0.,0.]]).T\n",
    "Y1 = np.array([[0.,6.]]).T\n",
    "P=L_layer_model_reversed(X1,Y1,parameters,layers_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_inputs = P[\"A0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.26553030e-04],\n",
       "       [  4.21928661e-04],\n",
       "       [  1.72517850e-05]])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
